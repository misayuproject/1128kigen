{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misayuproject/1128kigen/blob/main/try1125.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au9GMhtndzYu",
        "outputId": "45a3d7f1-b0cc-4685-b097-66f774895f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#ドライブの接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1_T2rr4vP-X"
      },
      "source": [
        "11月25日\n",
        "\n",
        "SHAPで特徴量の寄与率を調べたい\n",
        "そのためには、アンサンブルとLightGMBのみの評価と、切り替えられるようにしておきたい。LightGBMのみで一度評価して、それをSHAPで調べて、上位のみにしてアンサンブルかけるとか？\n",
        "\n",
        "とりあえずアンサンブルがきれいに書き直せたので、この後の作戦を考えよう\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRNJVXemAGg9"
      },
      "outputs": [],
      "source": [
        "#　18秒\n",
        "\n",
        "#コアデータ処理\n",
        "import pandas as pd                         # データを表のように扱うライブラリ\n",
        "import numpy as np                          # 数値計算を速くするライブラリ\n",
        "from datetime import datetime               #日付と時刻を扱うための基本的なクラス\n",
        "\n",
        "# テキスト処理と感情分析\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer       #このツールは、テキストで表現されている感情的なトーン（ポジティブ、ネガティブ、ニュートラル）を判断するのに役立ちます。\n",
        "from textblob import TextBlob               # 感情分析などの機能を提供するテキスト処理ライブラリ\n",
        "\n",
        "# 特徴量エンジニアリングと前処理:\n",
        "from scipy.sparse import hstack             # スパース行列(多くのゼロを含むデータを効率的に格納する方法であるスパース行列を操作する)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDFと呼ばれる手法を使用してテキストを数値ベクトルに変換します。これは、ドキュメント内の単語の重要性を表すのに役立ちます。\n",
        "from sklearn.feature_extraction.text import CountVectorizer # テキストデータ内の単語の出現回数を単純にカウントすることによって数値ベクトルを作成\n",
        "from sklearn.decomposition import TruncatedSVD  # 次元削減に使用され、データの特徴の数を減らします。\n",
        "from sklearn.model_selection import train_test_split  # データを訓練用と検証用に分けるために使う。機械学習モデルの性能を評価するために不可欠\n",
        "from sklearn.preprocessing import StandardScaler    # 数値特徴を平均0、分散1になるようにスケーリングすることで標準化します\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder   #テゴリデータを数値形式にエンコードするために使用\n",
        "\n",
        "import gensim.downloader as api             # 事前学習済みモデルをダウンロードするための api をインポート\n",
        "\n",
        "#予測モデルに関するライブラリ\n",
        "import lightgbm as lgb                      # 勾配ブースティングフレームワーク\n",
        "import xgboost as xgb                       # もう1つの強力な勾配ブースティングライブラリ\n",
        "!pip install catboost                       # もう1つの勾配ブースティングライブラリ\n",
        "from catboost import CatBoostRegressor      # CatBoostRegressor は、特にその回帰モデル（連続値の予測）を指します\n",
        "from sklearn.ensemble import RandomForestRegressor  # これは、分類タスクと回帰タスクの両方によく使用\n",
        "from sklearn.model_selection import KFold   #データをk個のフォールド（グループ）に分割し、交差検証を行うためのクラス\n",
        "\"\"\"\n",
        "動作:\n",
        "1.データセットをk個のフォールドに分割します。\n",
        "2.k回の学習と評価を繰り返します。\n",
        "    各回で、k-1個のフォールドを学習データ、残りの1個のフォールドを検証データとして使用します。\n",
        "    学習データでモデルを学習し、検証データで性能を評価します。\n",
        "3.k回の評価結果を平均して、モデルの最終的な性能を評価します。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error # 平均二乗誤差 (MSE) を計算する関数\n",
        "\"\"\"\n",
        "計算式　MSE = (1/n) * Σ(yi - ŷi)^2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import re   #正規表現モジュール re、正規表現は、テキスト内で特定のパターンを検索したり、操作したりするための強力なツールです。\n",
        "\"\"\"\n",
        "re の用途:\n",
        "パターンマッチング: テキスト内で特定のパターン (例えば、メールアドレス、電話番号、日付) を見つける。\n",
        "テキストの置換: テキスト内の特定のパターンを別のテキストに置き換える。\n",
        "テキストの抽出: テキストから特定のパターンに一致する部分を抽出する。\n",
        "データクリーニング: データセット内のテキストデータをクリーニングし、標準化する (例えば、不要な空白を削除する、大文字と小文字を統一する)。\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')  # 不要な警告を表示しない\n",
        "\"\"\"\n",
        "1.import warnings: Pythonに組み込まれているwarningsモジュールをインポートします。このモジュールは、警告メッセージを管理するための機能を提供します。警告メッセージは、エラーほど深刻ではありませんが、潜在的な問題や非推奨の機能の使用などを示すために使われます。\n",
        "2.warnings.simplefilter('ignore'): この行が実際に警告メッセージを非表示にする処理を行います。\n",
        "    warnings.simplefilter() は、警告メッセージに対するフィルターを設定する関数です。\n",
        "    'ignore' という引数を指定することで、全ての警告メッセージを無視するように設定しています。つまり、警告メッセージはコンソールやログに出力されなくなります。\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rcfo6jrpJBTF"
      },
      "outputs": [],
      "source": [
        "#データ読み込みセクション　8秒ーーーーーーーーーーーーーーーー\n",
        "#予測モデルを訓練するためのデータセット\n",
        "train = pd.read_csv('/content/drive/MyDrive/data/train.csv', index_col=0)\n",
        "# 予測モデルに推論（予測)させるデータセット\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/test.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストをクリーニングする関数　preprocess_cleanの定義　12秒\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "def preprocess_clean(train, test, columns):\n",
        "    def clean_text(text):\n",
        "        if pd.isnull(text):\n",
        "            return ''\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "    for col in columns:\n",
        "        if col in train.columns:\n",
        "            train[col] = train[col].fillna('').map(clean_text)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in train DataFrame\")\n",
        "\n",
        "        if col in test.columns:\n",
        "            test[col] = test[col].fillna('').map(clean_text)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in test DataFrame\")\n",
        "\n",
        "    return train, test\n",
        "\n",
        "# 実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train, test = preprocess_clean(train, test, review_columns)\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End  : {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessingTime: {processing_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYRwyR5y42LD",
        "outputId": "92b8f8f7-2ec1-4a52-bb95-af023e342cd3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 2024-11-25 10:37:58.458304\n",
            "End: 2024-11-25 10:38:10.230320\n",
            "ProcessingTime: 0:00:11.772016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvwtnrRw9vcq",
        "outputId": "8184a1ef-e3ce-4b79-8a4d-fdfe181a5139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 2024-11-25 10:48:28.860813\n",
            "add_bow_features  : 2024-11-25 10:48:34.837604 - End\n",
            "add_ngram_features : 2024-11-25 10:48:54.265544 - End\n",
            "add_tfidf_features : 2024-11-25 10:49:06.181150 - End\n",
            "add_embedding_features : 2024-11-25 10:50:53.947584 - End\n",
            "add_polarity_features : 2024-11-25 10:52:24.973822 - End\n",
            "Positive_Review : 2024-11-25 10:52:24.973947 - End\n",
            "add_bow_features  : 2024-11-25 10:52:31.083427 - End\n",
            "add_ngram_features : 2024-11-25 10:52:52.168540 - End\n",
            "add_tfidf_features : 2024-11-25 10:53:03.781793 - End\n",
            "add_embedding_features : 2024-11-25 10:54:52.348736 - End\n",
            "add_polarity_features : 2024-11-25 10:56:20.837588 - End\n",
            "Negative_Review : 2024-11-25 10:56:20.837708 - End\n",
            "preprocess_reviews2 : 2024-11-25 10:56:40.804565 - End\n",
            "ProcessingTime: 0:08:11.943913\n"
          ]
        }
      ],
      "source": [
        "# Positive_ReviewとNegative_Reviewの内容をいろいろな手法で分析する.\n",
        "# BOWN,N-gram,TF-IDFの実行、埋め込み表現　・・・\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "def preprocess_reviews(train, test, columns):\n",
        "\n",
        "    # Bag-of-Words特徴量の追加　add_bow_features関数\n",
        "    \"\"\"\n",
        "    CountVectorizerを使用して、テキストデータを単語の出現回数に基づいた数値ベクトルに変換します。\n",
        "    stop_words='english'で英語のストップワード（一般的な単語）を削除します。\n",
        "    max_features=100で出現頻度が高い上位100単語のみを特徴量として使用します。\n",
        "    生成されたBag-of-Words特徴量は、trainとtestデータフレームに新しいカラムとして追加されます。\n",
        "    \"\"\"\n",
        "    def add_bow_features(train, test, column, max_features=10):\n",
        "        # Bag of Words特徴量の追加\n",
        "        bow = CountVectorizer(stop_words='english', max_features=10)    #max_features=50減らす\n",
        "        # 既にクリーニング済みのカラムデータをそのまま使用\n",
        "        train_bow = bow.fit_transform(train[column]).toarray()  # 変更箇所\n",
        "        test_bow = bow.transform(test[column]).toarray()       # 変更箇所\n",
        "\n",
        "        for i in range(train_bow.shape[1]):\n",
        "            train[f'{column}_bow_{i}'] = train_bow[:, i]\n",
        "            test[f'{column}_bow_{i}'] = test_bow[:, i]\n",
        "\n",
        "        return train, test\n",
        "\n",
        "\n",
        "    # テキストデータから N-gram 特徴量を抽出し、データフレームに追加する関数の定義\n",
        "    \"\"\"\n",
        "    N-gram とは、テキスト中に出現する連続した単語の組み合わせのことです。例えば、「the quick brown fox」という文から、\n",
        "        uni-gram (1-gram): the, quick, brown, fox\n",
        "        bi-gram (2-gram): the quick, quick brown, brown fox\n",
        "        tri-gram (3-gram): the quick brown, quick brown fox\n",
        "    といった N-gram を抽出できます。これらの N-gram を特徴量として使用することで、機械学習モデルはテキストデータのより深い意味を理解できるようになります。\n",
        "    CountVectorizer を使用し、uni-gram と bi-gram を抽出することで、単語の出現頻度だけでなく、単語の組み合わせも考慮した特徴量を作成している。\n",
        "    この関数は、指定されたカラム (column) のテキストデータから N-gram 特徴量を抽出し、データフレームに追加します。\n",
        "    ngram_range: N-gram の範囲を指定します。デフォルトは (1, 2) で、uni-gram と bi-gram を抽出します。\n",
        "    max_features: 追加する特徴量の上限を設定します。デフォルトは 100 で、出現頻度が高い上位 100 個の N-gram のみを特徴量として使用します。\n",
        "    \"\"\"\n",
        "    def add_ngram_features(df, column, ngram_range=(1, 2), max_features=50):  # max_features = 50\n",
        "\n",
        "        vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)  # max_features を設定\n",
        "        # 既にクリーニング済みのカラムデータをそのまま使用\n",
        "        ngram_features = vectorizer.fit_transform(df[column])  # 変更箇所\n",
        "\n",
        "        # 特徴量名をデータフレームに追加\n",
        "        ngram_feature_names = vectorizer.get_feature_names_out()  # 抽出された特徴量の名前を取得します。\n",
        "        for i, feature_name in enumerate(ngram_feature_names):\n",
        "            # 抽出された各 N-gram 特徴量を、新しいカラムとしてデータフレームに追加します。\n",
        "            df[f'{column}_ngram_{feature_name}'] = ngram_features[:, i].toarray().ravel()\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    # TF-IDF特徴量を追加する関数(定義)\n",
        "    \"\"\"\n",
        "    TF-IDFベクトライザーの適用: TfidfVectorizerを使用して、テキストデータをTF-IDF値に基づいた数値ベクトルに変換します。\n",
        "\n",
        "    max_features=1000で出現頻度が高い上位1000単語のみを特徴量として使用します。\n",
        "    stop_words='english'で英語のストップワードを削除します。\n",
        "    特徴量の次元削減: TruncatedSVDを使用して、TF-IDF特徴量の次元数を削減します。\n",
        "\n",
        "    n_components=20で次元数を20に削減します。\n",
        "    random_state=42で結果の再現性を確保します。\n",
        "    特徴量をデータフレームに追加: 生成されたTF-IDF特徴量は、trainとtestデータフレームに新しいカラムとして追加されます。\n",
        "\n",
        "    関数の適用: add_tfidf_features関数を'Positive_Review'と'Negative_Review'カラムに適用し、TF-IDF特徴量を追加します。\n",
        "    \"\"\"\n",
        "    def add_tfidf_features(train, test, column, n_components=100, max_features=100):\n",
        "\n",
        "        # TF-IDFベクトライザーの適用\n",
        "        tfidf = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
        "        tfidf_train = tfidf.fit_transform(train[column])\n",
        "        tfidf_test = tfidf.transform(test[column])\n",
        "\n",
        "        # 特徴量の次元削減\n",
        "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "        svd_train = svd.fit_transform(tfidf_train)\n",
        "        svd_test = svd.transform(tfidf_test)\n",
        "\n",
        "        # 特徴量をデータフレームに追加\n",
        "        for i in range(n_components):\n",
        "            train[f'TFIDF_{column}_{i}'] = svd_train[:, i]\n",
        "            test[f'TFIDF_{column}_{i}'] = svd_test[:, i]\n",
        "\n",
        "        return train, test\n",
        "\n",
        "\n",
        "    # 埋め込み表現をデータフレームに追加する関数定義\n",
        "    def add_embedding_features(df, column, model_name='glove-wiki-gigaword-100', max_features=50): # max_features を引数に追加\n",
        "\n",
        "        # glove-wiki-gigaword-100 という名前の事前学習済みモデル (GloVe) を読み込みます。\n",
        "        # このモデルは、Wikipediaなどの大規模なテキストデータから単語の埋め込み表現を学習しています。\n",
        "        model = api.load(model_name)  # 事前学習済みモデルの読み込み\n",
        "\n",
        "        # テキストを受け取り、その埋め込み表現を返す関数を定義\n",
        "        def get_embedding(text):\n",
        "            words = text.split()    # テキストを単語に分割\n",
        "            embeddings = [model[word] for word in words if word in model]   #各単語の埋め込み表現を事前学習済みモデルから取得\n",
        "\n",
        "            if embeddings:          # 単語の埋め込み表現が存在する場合、それらを平均してテキスト全体の埋め込み表現を計算\n",
        "                return np.mean(embeddings, axis=0)\n",
        "            else:                   # 単語の埋め込み表現が存在しない場合、ゼロベクトルを返す\n",
        "                return np.zeros(model.vector_size)\n",
        "\n",
        "        # get_embedding 関数を df の指定された column ( 'Positive_Review' または 'Negative_Review' ) に適用し、\n",
        "        # 埋め込み表現をデータフレームに変換する\n",
        "        embedding_features = df[column].apply(get_embedding).apply(pd.Series)\n",
        "\n",
        "        # 埋め込み表現の特徴量数を max_features で指定された数に制限します。\n",
        "        embedding_features = embedding_features.iloc[:, :max_features] # 列数を制限\n",
        "\n",
        "        # 埋め込み表現のカラムに名前を付けます (例: 'Positive_Review_embedding_0', 'Positive_Review_embedding_1', ...)。\n",
        "        embedding_features.columns = [f'{column}_embedding_{i}' for i in range(embedding_features.shape[1])]\n",
        "\n",
        "        # 埋め込み表現を元のデータフレームに結合します。\n",
        "        df = pd.concat([df, embedding_features], axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    # 極性と主観性をデータフレームに追加する関数の定義 合計4カラム追加される。\n",
        "    def add_polarity_features(df, column):\n",
        "\n",
        "        # TextBlob を使って、指定された column ( 'Positive_Review' または 'Negative_Review' ) のテキストの極性を計算し、{column}_polarity という名前の新しいカラムに追加します。\n",
        "        df[f'{column}_polarity'] = df[column].apply(lambda x: TextBlob(column).sentiment.polarity)\n",
        "\n",
        "        # 同様に、主観性を計算し、{column}_subjectivity という名前の新しいカラムに追加します。\n",
        "        df[f'{column}_subjectivity'] = df[column].apply(lambda x: TextBlob(column).sentiment.subjectivity)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    # 5種の関数を呼び出す ['Positive_Review', 'Negative_Review']の二回\n",
        "    \"\"\"\n",
        "    add_ngram_features 関数: データフレーム df に直接カラムを追加するため、return df がなくても元のデータフレームに反映される。\n",
        "    add_embedding_features 関数: 新しいデータフレームを作成して df に代入するため、return df で更新された df を返す必要がある。\n",
        "    \"\"\"\n",
        "    for col in columns:\n",
        "\n",
        "        # add_bow_features関数実行\n",
        "        train, test = add_bow_features(train, test, col, max_features=50)\n",
        "        print(f\"add_bow_features  : {datetime.now()} - End\")\n",
        "\n",
        "        # N-gram 特徴量を抽出関数実行\n",
        "        train = add_ngram_features(train, col, max_features=100)    # 使用例: 上位 〇〇 個の Ngram 特徴量のみを追加\n",
        "        test = add_ngram_features(test, col, max_features=100)     # 使用例: 上位 〇〇 個の Ngram 特徴量のみを追加\n",
        "        print(f\"add_ngram_features : {datetime.now()} - End\")\n",
        "\n",
        "        # TF-IDF特徴量追加関数実行 +200\n",
        "        train, test = add_tfidf_features(train, test, col, max_features=100)\n",
        "        print(f\"add_tfidf_features : {datetime.now()} - End\")\n",
        "\n",
        "        # 埋め込み表現追加関数実行 +200\n",
        "        train = add_embedding_features(train, col, max_features=100)\n",
        "        test = add_embedding_features(test, col, max_features=100)\n",
        "        print(f\"add_embedding_features : {datetime.now()} - End\")\n",
        "\n",
        "        # 極性と主観性を追加する関数実行 +4\n",
        "        train = add_polarity_features(train, col)\n",
        "        test = add_polarity_features(test, col)\n",
        "        print(f\"add_polarity_features : {datetime.now()} - End\")\n",
        "\n",
        "        print(f\"{col} : {datetime.now()} - End\")\n",
        "\n",
        "    # preprocess_reviews関数終わり\n",
        "    return train, test\n",
        "\n",
        "# preprocess_reviews関数の実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train, test = preprocess_reviews(train, test, review_columns)\n",
        "\n",
        "\n",
        "# キーワードを抽出し、特徴量として追加する関数　⁺100\n",
        "def preprocess_reviews2(df, max_features=100):\n",
        "    def extract_keywords(text):\n",
        "        words = text.lower().split()\n",
        "        common_words = ['the', 'and', 'is', 'in', 'it', 'to', 'was', 'i', 'of', 'a', 'this']\n",
        "        keywords = [word for word in words if word not in common_words]\n",
        "        return ' '.join(keywords)\n",
        "\n",
        "    df['Positive_Review_Keywords'] = df['Positive_Review'].apply(extract_keywords)\n",
        "    df['Negative_Review_Keywords'] = df['Negative_Review'].apply(extract_keywords)\n",
        "\n",
        "    # CountVectorizerを使ってキーワードをエンコード（フィーチャースペースを制限）\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "\n",
        "    positive_keywords_matrix = vectorizer.fit_transform(df['Positive_Review_Keywords'])\n",
        "    negative_keywords_matrix = vectorizer.fit_transform(df['Negative_Review_Keywords'])\n",
        "\n",
        "    # スパース行列を結合\n",
        "    keywords_matrix = hstack([positive_keywords_matrix, negative_keywords_matrix])\n",
        "\n",
        "    # スパース行列をデータフレームに変換する\n",
        "    keywords_df = pd.DataFrame(keywords_matrix.toarray(), columns=[f'Keyword_{i}' for i in range(keywords_matrix.shape[1])])\n",
        "\n",
        "    # 元のデータフレームに結合\n",
        "    df = pd.concat([df.reset_index(drop=True), keywords_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# キーワードを抽出し、特徴量として追加する関数の実行\n",
        "train = preprocess_reviews2(train, max_features=100)\n",
        "test = preprocess_reviews2(test, max_features=100)\n",
        "print(f\"preprocess_reviews2 : {datetime.now()} - End\")\n",
        "\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End  : {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessingTime: {processing_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49hSgOoNQgRE",
        "outputId": "bdfbec2d-f48e-4f62-d5c2-953c16437548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 2024-11-25 10:57:08.159458\n",
            "End: 2024-11-25 10:59:06.295808\n",
            "ProcessTime: 0:01:58.136350\n"
          ]
        }
      ],
      "source": [
        "#Review項目以外の特徴量追加　#3、#4，#5\n",
        "#レビュー日付、ホテル名、緯度経度といった情報から新しい特徴量を生成\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "# 3. レビュー日付からの特徴量生成(定義)g\n",
        "def extract_date_features(df, date_column):\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "    df['review_weekday'] = df[date_column].dt.weekday   #レビューが投稿された曜日 (0:月曜日, 1:火曜日, ..., 6:日曜日)\n",
        "    df['review_month'] = df[date_column].dt.month       #レビューが投稿された月 (1:1月, 2:2月, ..., 12:12月)\n",
        "    df['review_quarter'] = df[date_column].dt.quarter   #レビューが投稿された四半期 (1:1-3月, 2:4-6月, 3:7-9月, 4:10-12月)\n",
        "    df['review_season'] = df[date_column].dt.month % 12 // 3 + 1    #レビューが投稿された季節 (1:春, 2:夏, 3:秋, 4:冬)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 3．レビュー日付からの特徴量生成(実行)\n",
        "date_column = 'Review_Date'\n",
        "train = extract_date_features(train, date_column)\n",
        "test = extract_date_features(test, date_column)\n",
        "\n",
        "\n",
        "# 4. ホテル名の処理(定義)\n",
        "def preprocess_hotel_names(train, test, column):\n",
        "    train[f'{column}_length'] = train[column].apply(len)    #Hotel_Name_length: ホテル名の文字数\n",
        "    test[f'{column}_length'] = test[column].apply(len)      #Hotel_Name_length: ホテル名の文字数\n",
        "\n",
        "    train[f'{column}_frequency'] = train[column].map(train[column].value_counts())  #Hotel_Name_frequency: データセット全体におけるそのホテル名の出現頻度\n",
        "    test[f'{column}_frequency'] = test[column].map(train[column].value_counts())    #Hotel_Name_frequency: データセット全体におけるそのホテル名の出現頻度\n",
        "\n",
        "    return train, test\n",
        "\n",
        "# 4．ホテル名の処理(実行)\n",
        "hotel_name_column = 'Hotel_Name'\n",
        "train, test = preprocess_hotel_names(train, test, hotel_name_column)\n",
        "\n",
        "\n",
        "# 5. 緯度・経度の組み合わせ(定義)\n",
        "#機械学習モデルがホテルの位置情報をより効果的に利用できるようにするために設計\n",
        "#train、testに、lat_lng と lat_lng_encoded という2つの新しいカラムを追加する\n",
        "\"\"\"\n",
        "1.緯度経度の結合:\n",
        "lat カラム（緯度）と lng カラム（経度）の値を文字列に変換し、\"_\" で連結して新しい lat_lng カラムを作成します。\n",
        "例えば、緯度が35.6895、経度が139.6917の場合、lat_lng カラムの値は \"35.6895_139.6917\" となります。\n",
        "これにより、緯度と経度を組み合わせた単一の特徴量が生成されます。\n",
        "2.ラベルエンコーディング:\n",
        "LabelEncoder を使用して、lat_lng カラムの値を数値に変換します。\n",
        "LabelEncoder は、カテゴリカルデータを数値に変換する際に使用される手法です。\n",
        "fit_transform メソッドを使用して、train データフレームの lat_lng カラムに含まれるユニークな値にそれぞれ数値を割り当てます。\n",
        "transform メソッドを使用して、test データフレームの lat_lng カラムの値を、train データフレームで割り当てられた数値に変換します。\n",
        "これにより、lat_lng_encoded という新しいカラムが作成され、緯度経度の組み合わせが数値で表現されます。\n",
        "\"\"\"\n",
        "def preprocess_lat_lng(train, test, lat_column, lng_column):\n",
        "    # K-meansクラスタリングの代わりに、簡単な組み合わせ特徴量を作成\n",
        "    train['lat_lng'] = train[lat_column].astype(str) + '_' + train[lng_column].astype(str)\n",
        "    test['lat_lng'] = test[lat_column].astype(str) + '_' + test[lng_column].astype(str)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    train['lat_lng_encoded'] = encoder.fit_transform(train['lat_lng'])\n",
        "    test['lat_lng_encoded'] = encoder.transform(test['lat_lng'])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "# 5．緯度・経度の組み合わせ(実行)\n",
        "lat_column = 'lat'\n",
        "lng_column = 'lng'\n",
        "train, test = preprocess_lat_lng(train, test, lat_column, lng_column)\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "\n",
        "    # レビューの文字数を計算する\n",
        "    df['Positive_Review_Length'] = df['Positive_Review'].apply(len)\n",
        "    df['Negative_Review_Length'] = df['Negative_Review'].apply(len)\n",
        "\n",
        "    # レビューのポジティブ・ネガティブ比率\n",
        "    df['Review_Word_Ratio'] = df['Positive_Review_Length'] / (df['Negative_Review_Length'] + 1)\n",
        "\n",
        "    # ポジティブレビューの単語数を計算\n",
        "    df['Positive_Review_Word_Count'] = df['Positive_Review'].apply(lambda x: len(x.split()))\n",
        "    # ネガティブレビューの単語数を計算\n",
        "    df['Negative_Review_Word_Count'] = df['Negative_Review'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    # VADERを使ったセンチメント分析\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    df['Positive_Review_Sentiment'] = df['Positive_Review'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "    df['Negative_Review_Sentiment'] = df['Negative_Review'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "\n",
        "    # 過去のレビュー数と平均スコア\n",
        "    df['Reviews_Per_Hotel'] = df.groupby('Hotel_Name')['Total_Number_of_Reviews'].transform('sum')\n",
        "    df['Average_Score_Per_Hotel'] = df.groupby('Hotel_Name')['Average_Score'].transform('mean')\n",
        "\n",
        "    # ホテルのレビュー数の変化\n",
        "    df['Review_Trend'] = df.groupby('Hotel_Name')['Total_Number_of_Reviews'].transform(lambda x: x.diff().fillna(0))\n",
        "    df['Average_Score_Trend'] = df.groupby('Hotel_Name')['Average_Score'].transform(lambda x: x.diff().fillna(0))\n",
        "\n",
        "    # レビュアーのアクティブ度\n",
        "    df['Reviewer_Activity'] = df['Total_Number_of_Reviews_Reviewer_Has_Given'] / df['days_since_review'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "    # 月ごとの宿泊者満足度\n",
        "    if 'Reviewer_Score' in df.columns:\n",
        "        df['Monthly_Avg_Score'] = df.groupby('review_month')['Reviewer_Score'].transform('mean')\n",
        "\n",
        "    # 否定的・肯定的なレビューの有無\n",
        "    df['Has_Negative_Review'] = df['Negative_Review'].apply(lambda x: 1 if x.strip() else 0)\n",
        "    df['Has_Positive_Review'] = df['Positive_Review'].apply(lambda x: 1 if x.strip() else 0)\n",
        "\n",
        "    # レビュアーの詳細情報解析\n",
        "    df['Trip_Type'] = df['Tags'].apply(lambda x: 1 if 'Leisure trip' in x else 0)\n",
        "    df['With_Family'] = df['Tags'].apply(lambda x: 1 if 'Family with young children' in x or 'Family with older children' in x else 0)\n",
        "    df['With_Partner'] = df['Tags'].apply(lambda x: 1 if 'Couple' in x else 0)\n",
        "    df['Solo_Traveler'] = df['Tags'].apply(lambda x: 1 if 'Solo traveler' in x else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "#6．もとから作ってた前処理を実行　2分17秒\n",
        "train = preprocess_data(train)\n",
        "test = preprocess_data(test)\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End  : {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1qDa5Sn_XNz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYLWwxfjReWs",
        "outputId": "6206d27e-cf1d-4dc4-9101-6551d0d6d8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(283366, 867)\n",
            "(231845, 865)\n"
          ]
        }
      ],
      "source": [
        "#6まで全体のバックアップ取得\n",
        "train_bkup6 = train.copy()\n",
        "test_bkup6 = test.copy()\n",
        "\n",
        "print(train_bkup6.shape)\n",
        "print(test_bkup6.shape)\n",
        "\n",
        "#(283366, 327)\n",
        "#(231845, 325)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ddnBwQZ-jn"
      },
      "outputs": [],
      "source": [
        "#バックアップから戻す\n",
        "train = train_bkup6.copy()\n",
        "test = test_bkup6.copy()\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XScmbuIHdpD"
      },
      "outputs": [],
      "source": [
        "#アンサンブルコード書き直し　11月25日\n",
        "#ランダムフォレスト除外\n",
        "#3つのモデルのハイパーパラメータ変更\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "# 必要なモジュールのインポートを追加→結局使わない\n",
        "#from xgboost.callback import EarlyStopping\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "# 訓練用データセットの数値型以外の列を削除する\n",
        "train = train.select_dtypes(include=['int64', 'float64', 'int32'])\n",
        "\n",
        "# 推論用データセットにもレビュースコア以外の同様の列を残す\n",
        "test = test[[col for col in train.columns if col in test.columns]]\n",
        "\n",
        "# 訓練用データセットからターゲットを分離する\n",
        "X = train.drop('Reviewer_Score', axis=1)\n",
        "y = train['Reviewer_Score']\n",
        "\n",
        "# テストデータに存在しない列を追加して0で埋める\n",
        "missing_cols = set(X.columns) - set(test.columns)\n",
        "for col in missing_cols:\n",
        "    test[col] = 0\n",
        "\n",
        "\n",
        "# 列の順番を訓練データと同じにする\n",
        "test = test[X.columns]\n",
        "\n",
        "# 無限大の値を置換\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 欠損値を平均値で補完 (他の方法でも補完可能)\n",
        "X = X.fillna(X.mean())            #平均値で埋める\n",
        "test = test.fillna(test.mean())\n",
        "# 例：X_train = X_train.dropna()  # NaN値を含む行を削除\n",
        "\n",
        "# float64に型変換\n",
        "X = X.astype(np.float64)\n",
        "test = test.astype(np.float64)\n",
        "\n",
        "\n",
        "# KFoldによる分割設定\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 結果を保存するリスト\n",
        "rmses = []\n",
        "test_preds_lgb = np.zeros(test.shape[0])\n",
        "test_preds_rf = np.zeros(test.shape[0])\n",
        "test_preds_xgb = np.zeros(test.shape[0])\n",
        "test_preds_cat = np.zeros(test.shape[0])\n",
        "\n",
        "# 各モデルの最適なハイパーパラメータの設定\n",
        "lgb_params = {\n",
        "    'learning_rate': 0.1,   #0.05\n",
        "    'min_child_samples': 50,\n",
        "    'n_estimators': 1000,   #500\n",
        "    'num_leaves': 70,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\"\"\"\n",
        "rf_params = {\n",
        "    'n_estimators': 25, #ランダムフォレストを構成する決定木の数,100〜200 程度から始め、交差検証などで最適な値を探索するのが良い\n",
        "    'random_state': 42, #乱数シード\n",
        "    'max_depth': 10,    #各決定木の最大深さ None（制限なし）から始め、交差検証などで最適な値を探索するのが良い\n",
        "    'min_samples_split': 2, #ノードを分割するために必要な最小サンプル数 2〜10 程度から始め、交差検証などで最適な値を探索するのが良い\n",
        "    'min_samples_leaf': 1,  #葉ノードに存在する最小サンプル数 1〜5 程度から始め、交差検証などで最適な値を探索するのが良い\n",
        "    #'max_features': 'auto',    #各決定木でノードを分割する際に考慮する特徴量の最大数 'auto'（特徴量の数の平方根）、'sqrt'、'log2' など\n",
        "    'n_jobs': -1    #並列処理に使用するコア数  -1 を指定すると、すべてのコアを使用\n",
        "}\n",
        "\"\"\"\n",
        "xgb_params = {\n",
        "    'learning_rate': 0.1,  #デフォルトは0.3　一般的に、0.01〜0.1程度の値が使用される。\n",
        "    'n_estimators': 1000,    #ブースター（決定木）の数\n",
        "    'objective': 'reg:squarederror',    #reg:squarederror（回帰）\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "cat_params = {\n",
        "    'iterations': 500,      #学習の反復回数 (決定木の数)\n",
        "    'learning_rate': 0.1,  #学習率\n",
        "    'depth': 6,             #決定木の最大深度\n",
        "    'loss_function': 'RMSE',#損失関数\n",
        "    'random_seed': 42,      #乱数シード\n",
        "    'verbose': 0            #学習過程の出力レベル　0表示なし　1表示あり\n",
        "}\n",
        "\n",
        "# 各foldでの訓練と評価\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X)):\n",
        "    print(f\"Fold {fold+1} Start:{datetime.now()}\")\n",
        "\n",
        "    # 訓練用データと検証用データに分割\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    # ランダムフォレストモデルの訓練\n",
        "    #rf_model = RandomForestRegressor(**rf_params)\n",
        "    #rf_model.fit(X_train, y_train)\n",
        "    #print(f\"Fold {fold+1} ランダムフォレスト time:{datetime.now()}\")\n",
        "\n",
        "    # LightGBMモデルの訓練\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "    print(f\"Fold {fold+1} LightGBM time:{datetime.now()}\")\n",
        "\n",
        "\n",
        "    # XGBoostモデルの訓練\n",
        "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "    early_stop = EarlyStopping(rounds=50) # EarlyStoppingコールバックを作成\n",
        "    #xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stop], verbose=False) # callbacks引数に追加\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
        "    print(f\"Fold {fold+1} XGBoost time:{datetime.now()}\")\n",
        "\n",
        "    # CatBoostモデルの訓練\n",
        "    cat_model = CatBoostRegressor(**cat_params)\n",
        "    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, verbose=False)\n",
        "    print(f\"Fold {fold+1} CatBoost time:{datetime.now()}\")\n",
        "\n",
        "    # 検証用データセットに対する予測\n",
        "    y_pred_lgb = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
        "    #y_pred_rf = rf_model.predict(X_valid)\n",
        "    y_pred_xgb = xgb_model.predict(X_valid)\n",
        "    y_pred_cat = cat_model.predict(X_valid)\n",
        "\n",
        "    # モデルの性能をRMSEで評価\n",
        "    rmse_lgb = mean_squared_error(y_valid, y_pred_lgb, squared=False)\n",
        "    #rmse_rf = mean_squared_error(y_valid, y_pred_rf, squared=False)\n",
        "    rmse_xgb = mean_squared_error(y_valid, y_pred_xgb, squared=False)\n",
        "    rmse_cat = mean_squared_error(y_valid, y_pred_cat, squared=False)\n",
        "    print(f\"Fold {fold+1} RMSE (LightGBM): {rmse_lgb:.4f}\")\n",
        "    #print(f\"Fold {fold+1} RMSE (RandomForest): {rmse_rf:.4f}\")\n",
        "    print(f\"Fold {fold+1} RMSE (XGBoost): {rmse_xgb:.4f}\")\n",
        "    print(f\"Fold {fold+1} RMSE (CatBoost): {rmse_cat:.4f}\")\n",
        "\n",
        "    # 各foldの結果を保存\n",
        "    #rmses.append((rmse_lgb, rmse_rf, rmse_xgb, rmse_cat))\n",
        "    rmses.append((rmse_lgb, rmse_xgb, rmse_cat))\n",
        "\n",
        "    # テストデータに対する予測を平均\n",
        "    test_preds_lgb += lgb_model.predict(test, num_iteration=lgb_model.best_iteration) / kf.n_splits\n",
        "    #test_preds_rf += rf_model.predict(test) / kf.n_splits\n",
        "    test_preds_xgb += xgb_model.predict(test) / kf.n_splits\n",
        "    test_preds_cat += cat_model.predict(test) / kf.n_splits\n",
        "\n",
        "# 全体の平均RMSE\n",
        "mean_rmse_lgb = sum([rmse[0] for rmse in rmses]) / len(rmses)\n",
        "#mean_rmse_rf = sum([rmse[1] for rmse in rmses]) / len(rmses)\n",
        "mean_rmse_xgb = sum([rmse[2] for rmse in rmses]) / len(rmses)\n",
        "mean_rmse_cat = sum([rmse[3] for rmse in rmses]) / len(rmses)\n",
        "print(f\"Mean RMSE (LightGBM): {mean_rmse_lgb:.4f}\")\n",
        "#print(f\"Mean RMSE (RandomForest): {mean_rmse_rf:.4f}\")\n",
        "print(f\"Mean RMSE (XGBoost): {mean_rmse_xgb:.4f}\")\n",
        "print(f\"Mean RMSE (CatBoost): {mean_rmse_cat:.4f}\")\n",
        "\n",
        "# アンサンブル予測の平均\n",
        "final_test_preds = (test_preds_lgb + test_preds_rf + test_preds_xgb + test_preds_cat) / 4\n",
        "final_test_preds = (test_preds_lgb + test_preds_xgb + test_preds_cat) / 3\n",
        "\n",
        "# 予測結果を保存する\n",
        "submit = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv', header=None)\n",
        "submit[1] = final_test_preds\n",
        "submit.to_csv('/content/drive/MyDrive/data/submit_ensemble.csv', header=None, index=False)\n",
        "\n",
        "submit.head()\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End: {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna\n",
        "\n",
        "#learning_rate, num_leaves, max_depthはモデルの性能に大きく影響するため、優先的にチューニングすることをお勧めします。\n",
        "def objective(trial):\n",
        "    # ハイパーパラメータの範囲を指定\n",
        "    param = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        # ... other hyperparameters ...\n",
        "    }\n",
        "\n",
        "    # LightGBMモデルの学習\n",
        "    lgb_model = lgb.train(\n",
        "        param,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "\n",
        "    # 予測とRMSEの計算\n",
        "    y_pred = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
        "    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "\n",
        "    return rmse  # 最小化したい指標を返す\n",
        "\n",
        "study = optuna.create_study(direction='minimize')  # 最小化したい場合は 'minimize'\n",
        "study.optimize(objective, n_trials=100)  # 試行回数\n",
        "\n",
        "# 最適なハイパーパラメータ\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxM7toEDI56K",
        "outputId": "a1b9ecc1-fd67-4e4e-d7cd-ce66258601ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:12:01,443] A new study created in memory with name: no-name-75506969-ef45-4bdc-89c5-2bd4af00e89a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.13216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:12:37,887] Trial 0 finished with value: 1.064029094718614 and parameters: {'learning_rate': 0.032854689317553296, 'num_leaves': 69, 'min_child_samples': 14}. Best is trial 0 with value: 1.064029094718614.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.14361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:13:42,175] Trial 1 finished with value: 1.0693980160917085 and parameters: {'learning_rate': 0.025887384126961625, 'num_leaves': 141, 'min_child_samples': 16}. Best is trial 0 with value: 1.064029094718614.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 2.11799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:14:55,909] Trial 2 finished with value: 1.4553308792067672 and parameters: {'learning_rate': 0.0027366493764016883, 'num_leaves': 117, 'min_child_samples': 95}. Best is trial 0 with value: 1.064029094718614.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.86951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:16:24,452] Trial 3 finished with value: 1.3673013768040319 and parameters: {'learning_rate': 0.004402762447775082, 'num_leaves': 209, 'min_child_samples': 80}. Best is trial 0 with value: 1.064029094718614.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.95973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:17:22,756] Trial 4 finished with value: 1.3999023696928432 and parameters: {'learning_rate': 0.0037382098189221273, 'num_leaves': 171, 'min_child_samples': 70}. Best is trial 0 with value: 1.064029094718614.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.03277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:18:21,444] Trial 5 finished with value: 1.016252974940816 and parameters: {'learning_rate': 0.07221446042689698, 'num_leaves': 226, 'min_child_samples': 96}. Best is trial 5 with value: 1.016252974940816.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.9216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:18:56,184] Trial 6 finished with value: 1.3862167133790821 and parameters: {'learning_rate': 0.00436401006385197, 'num_leaves': 56, 'min_child_samples': 23}. Best is trial 5 with value: 1.016252974940816.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 2.05805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:20:09,730] Trial 7 finished with value: 1.434591921554168 and parameters: {'learning_rate': 0.002976974995463791, 'num_leaves': 273, 'min_child_samples': 8}. Best is trial 5 with value: 1.016252974940816.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.93227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:20:59,128] Trial 8 finished with value: 1.3900617838584932 and parameters: {'learning_rate': 0.004041153863291032, 'num_leaves': 116, 'min_child_samples': 63}. Best is trial 5 with value: 1.016252974940816.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 2.33237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:21:49,487] Trial 9 finished with value: 1.5272090614190057 and parameters: {'learning_rate': 0.001522684036657684, 'num_leaves': 133, 'min_child_samples': 9}. Best is trial 5 with value: 1.016252974940816.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.02963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:23:23,829] Trial 10 finished with value: 1.0147045134195116 and parameters: {'learning_rate': 0.08889626921971382, 'num_leaves': 290, 'min_child_samples': 38}. Best is trial 10 with value: 1.0147045134195116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.02622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:24:36,048] Trial 11 finished with value: 1.0130270400040866 and parameters: {'learning_rate': 0.09606566302262913, 'num_leaves': 298, 'min_child_samples': 45}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.03174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:25:43,988] Trial 12 finished with value: 1.015748019885015 and parameters: {'learning_rate': 0.08474221593434768, 'num_leaves': 292, 'min_child_samples': 40}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.12973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:26:52,949] Trial 13 finished with value: 1.0628858395107668 and parameters: {'learning_rate': 0.02508214372522857, 'num_leaves': 253, 'min_child_samples': 41}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.31901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:28:10,428] Trial 14 finished with value: 1.1484827522269523 and parameters: {'learning_rate': 0.012628086599098559, 'num_leaves': 300, 'min_child_samples': 37}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.05692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:29:12,525] Trial 15 finished with value: 1.0280664884563286 and parameters: {'learning_rate': 0.04659889969374741, 'num_leaves': 241, 'min_child_samples': 53}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.29735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:30:10,530] Trial 16 finished with value: 1.139011873208804 and parameters: {'learning_rate': 0.014015740489131519, 'num_leaves': 189, 'min_child_samples': 31}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's l2: 1.03029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:31:11,077] Trial 17 finished with value: 1.0150321632144361 and parameters: {'learning_rate': 0.0953919300594343, 'num_leaves': 262, 'min_child_samples': 54}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.05225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:32:10,275] Trial 18 finished with value: 1.0257910635766667 and parameters: {'learning_rate': 0.05032028527751928, 'num_leaves': 210, 'min_child_samples': 27}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.54865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:33:24,033] Trial 19 finished with value: 1.244446915982135 and parameters: {'learning_rate': 0.007783124971344095, 'num_leaves': 280, 'min_child_samples': 45}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.12822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:33:47,946] Trial 20 finished with value: 1.0621786938002753 and parameters: {'learning_rate': 0.052605883551681, 'num_leaves': 22, 'min_child_samples': 60}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.02642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:34:51,085] Trial 21 finished with value: 1.0131218311413512 and parameters: {'learning_rate': 0.09772467055697683, 'num_leaves': 259, 'min_child_samples': 51}. Best is trial 11 with value: 1.0130270400040866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.02602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:35:50,523] Trial 22 finished with value: 1.0129261790343862 and parameters: {'learning_rate': 0.09801746962815755, 'num_leaves': 252, 'min_child_samples': 48}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.07802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:36:55,577] Trial 23 finished with value: 1.0382790619161508 and parameters: {'learning_rate': 0.036693457441823195, 'num_leaves': 244, 'min_child_samples': 76}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.04202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:37:56,354] Trial 24 finished with value: 1.020793224044228 and parameters: {'learning_rate': 0.05990763387383375, 'num_leaves': 227, 'min_child_samples': 48}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.1767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:39:03,115] Trial 25 finished with value: 1.0847592587189643 and parameters: {'learning_rate': 0.02008935146831569, 'num_leaves': 268, 'min_child_samples': 62}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.03595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:39:58,624] Trial 26 finished with value: 1.017816940684662 and parameters: {'learning_rate': 0.06902428958702933, 'num_leaves': 202, 'min_child_samples': 50}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.02703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:40:51,024] Trial 27 finished with value: 1.013426406427234 and parameters: {'learning_rate': 0.09716565311860789, 'num_leaves': 177, 'min_child_samples': 69}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.08387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:41:52,918] Trial 28 finished with value: 1.0410905401344275 and parameters: {'learning_rate': 0.03551376476317772, 'num_leaves': 237, 'min_child_samples': 34}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.06033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:42:59,524] Trial 29 finished with value: 1.029722205587785 and parameters: {'learning_rate': 0.044618134722284986, 'num_leaves': 264, 'min_child_samples': 21}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.58982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:44:13,633] Trial 30 finished with value: 1.2608796763673995 and parameters: {'learning_rate': 0.007189265670164644, 'num_leaves': 300, 'min_child_samples': 57}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.02797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:45:12,416] Trial 31 finished with value: 1.013889418456129 and parameters: {'learning_rate': 0.09442949511002705, 'num_leaves': 181, 'min_child_samples': 68}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.05353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:45:54,483] Trial 32 finished with value: 1.0264158010984261 and parameters: {'learning_rate': 0.06816160602713207, 'num_leaves': 84, 'min_child_samples': 81}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.13726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:46:47,364] Trial 33 finished with value: 1.0664246026124675 and parameters: {'learning_rate': 0.02640381135742411, 'num_leaves': 151, 'min_child_samples': 88}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's l2: 1.03576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-25 12:47:50,854] Trial 34 finished with value: 1.017723169546828 and parameters: {'learning_rate': 0.06753961370601465, 'num_leaves': 227, 'min_child_samples': 70}. Best is trial 22 with value: 1.0129261790343862.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    param = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        # ... other hyperparameters ...\n",
        "    }\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(**param)\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
        "\n",
        "    y_pred = xgb_model.predict(X_valid)\n",
        "    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# 最適なハイパーパラメータ\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jymgDjuqNfqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b9mCfucL1KnG",
        "outputId": "0f446c76-ff23-4296-a833-c5f6c07edaf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found: {'learning_rate': 0.05, 'min_child_samples': 50, 'n_estimators': 500, 'num_leaves': 70}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# ハイパーパラメータの候補を設定\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 50, 70],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'min_child_samples': [20, 30, 50]\n",
        "}\n",
        "\n",
        "# LightGBMのモデルを設定\n",
        "lgb_model = lgb.LGBMRegressor(objective='regression', random_state=42)\n",
        "\n",
        "# GridSearchCVを使用して最適なハイパーパラメータを探索\n",
        "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# 最適なハイパーパラメータを表示\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    param = {\n",
        "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'depth': trial.suggest_int('depth', 4, 10),\n",
        "        # ... other hyperparameters ...\n",
        "    }\n",
        "\n",
        "    cat_model = CatBoostRegressor(**param, verbose=False)\n",
        "    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
        "\n",
        "    y_pred = cat_model.predict(X_valid)\n",
        "    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# 最適なハイパーパラメータ\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "I9r4QonPNhAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8d-GrfcwWkZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMEiwlD9gtASff3rWt3H3yT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}