{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misayuproject/1128kigen/blob/main/try1126.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au9GMhtndzYu"
      },
      "outputs": [],
      "source": [
        "#ドライブの接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1_T2rr4vP-X"
      },
      "source": [
        "11月26日\n",
        "\n",
        "SHAPで特徴量の寄与率を調べて下20パーセントを減らした後のLightGBMは精度が落ちる。→採用せずに行くか。\n",
        "\n",
        "特徴量の追加、\n",
        "\n",
        "CatBoosterのとXGBoosterのパラメータをOptuna使って調べるとか？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRNJVXemAGg9"
      },
      "outputs": [],
      "source": [
        "#　18秒\n",
        "\n",
        "#コアデータ処理\n",
        "import pandas as pd                         # データを表のように扱うライブラリ\n",
        "import numpy as np                          # 数値計算を速くするライブラリ\n",
        "from datetime import datetime               #日付と時刻を扱うための基本的なクラス\n",
        "\n",
        "# テキスト処理と感情分析\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer       #このツールは、テキストで表現されている感情的なトーン（ポジティブ、ネガティブ、ニュートラル）を判断するのに役立ちます。\n",
        "from textblob import TextBlob               # 感情分析などの機能を提供するテキスト処理ライブラリ\n",
        "\n",
        "# 特徴量エンジニアリングと前処理:\n",
        "from scipy.sparse import hstack             # スパース行列(多くのゼロを含むデータを効率的に格納する方法であるスパース行列を操作する)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDFと呼ばれる手法を使用してテキストを数値ベクトルに変換します。これは、ドキュメント内の単語の重要性を表すのに役立ちます。\n",
        "from sklearn.feature_extraction.text import CountVectorizer # テキストデータ内の単語の出現回数を単純にカウントすることによって数値ベクトルを作成\n",
        "from sklearn.decomposition import TruncatedSVD  # 次元削減に使用され、データの特徴の数を減らします。\n",
        "from sklearn.model_selection import train_test_split  # データを訓練用と検証用に分けるために使う。機械学習モデルの性能を評価するために不可欠\n",
        "from sklearn.preprocessing import StandardScaler    # 数値特徴を平均0、分散1になるようにスケーリングすることで標準化します\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder   #テゴリデータを数値形式にエンコードするために使用\n",
        "\n",
        "import gensim.downloader as api             # 事前学習済みモデルをダウンロードするための api をインポート\n",
        "\n",
        "#予測モデルに関するライブラリ\n",
        "import lightgbm as lgb                      # 勾配ブースティングフレームワーク\n",
        "import xgboost as xgb                       # もう1つの強力な勾配ブースティングライブラリ\n",
        "!pip install catboost                       # もう1つの勾配ブースティングライブラリ\n",
        "from catboost import CatBoostRegressor      # CatBoostRegressor は、特にその回帰モデル（連続値の予測）を指します\n",
        "from sklearn.ensemble import RandomForestRegressor  # これは、分類タスクと回帰タスクの両方によく使用\n",
        "from sklearn.model_selection import KFold   #データをk個のフォールド（グループ）に分割し、交差検証を行うためのクラス\n",
        "\"\"\"\n",
        "動作:\n",
        "1.データセットをk個のフォールドに分割します。\n",
        "2.k回の学習と評価を繰り返します。\n",
        "    各回で、k-1個のフォールドを学習データ、残りの1個のフォールドを検証データとして使用します。\n",
        "    学習データでモデルを学習し、検証データで性能を評価します。\n",
        "3.k回の評価結果を平均して、モデルの最終的な性能を評価します。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error # 平均二乗誤差 (MSE) を計算する関数\n",
        "\"\"\"\n",
        "計算式　MSE = (1/n) * Σ(yi - ŷi)^2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import re   #正規表現モジュール re、正規表現は、テキスト内で特定のパターンを検索したり、操作したりするための強力なツールです。\n",
        "\"\"\"\n",
        "re の用途:\n",
        "パターンマッチング: テキスト内で特定のパターン (例えば、メールアドレス、電話番号、日付) を見つける。\n",
        "テキストの置換: テキスト内の特定のパターンを別のテキストに置き換える。\n",
        "テキストの抽出: テキストから特定のパターンに一致する部分を抽出する。\n",
        "データクリーニング: データセット内のテキストデータをクリーニングし、標準化する (例えば、不要な空白を削除する、大文字と小文字を統一する)。\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')  # 不要な警告を表示しない\n",
        "\"\"\"\n",
        "1.import warnings: Pythonに組み込まれているwarningsモジュールをインポートします。このモジュールは、警告メッセージを管理するための機能を提供します。警告メッセージは、エラーほど深刻ではありませんが、潜在的な問題や非推奨の機能の使用などを示すために使われます。\n",
        "2.warnings.simplefilter('ignore'): この行が実際に警告メッセージを非表示にする処理を行います。\n",
        "    warnings.simplefilter() は、警告メッセージに対するフィルターを設定する関数です。\n",
        "    'ignore' という引数を指定することで、全ての警告メッセージを無視するように設定しています。つまり、警告メッセージはコンソールやログに出力されなくなります。\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rcfo6jrpJBTF"
      },
      "outputs": [],
      "source": [
        "#データ読み込みセクション　8秒ーーーーーーーーーーーーーーーー\n",
        "#予測モデルを訓練するためのデータセット\n",
        "train = pd.read_csv('/content/drive/MyDrive/data/train.csv', index_col=0)\n",
        "# 予測モデルに推論（予測)させるデータセット\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/test.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYRwyR5y42LD",
        "outputId": "ca259542-41c3-4ce6-ea83-11ccb199b840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 2024-11-26 01:46:53.566325\n",
            "train : (283366, 14) - End\n",
            "test : (231845, 13) - End\n",
            "End  : 2024-11-26 01:47:04.914438\n",
            "ProcessingTime: 0:00:11.348113\n"
          ]
        }
      ],
      "source": [
        "# テキストをクリーニングする関数　preprocess_cleanの定義　12秒\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "def preprocess_clean(train, test, columns):\n",
        "    def clean_text(text):\n",
        "        if pd.isnull(text):\n",
        "            return ''\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "    for col in columns:\n",
        "        if col in train.columns:\n",
        "            train[col] = train[col].fillna('').map(clean_text)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in train DataFrame\")\n",
        "\n",
        "        if col in test.columns:\n",
        "            test[col] = test[col].fillna('').map(clean_text)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in test DataFrame\")\n",
        "\n",
        "    return train, test\n",
        "\n",
        "# 実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train, test = preprocess_clean(train, test, review_columns)\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End  : {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessingTime: {processing_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvwtnrRw9vcq",
        "outputId": "d5d4fd4d-20b9-4610-faea-f524761c3d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 2024-11-26 01:47:15.436099\n",
            "add_bow_features  : 2024-11-26 01:47:21.219333 - End \n",
            "train : (283366, 24) - End\n",
            "test : (231845, 23) - End\n",
            "add_ngram_features : 2024-11-26 01:47:40.073792 - End\n",
            "train : (283366, 124) - End\n",
            "test : (231845, 123) - End\n",
            "add_tfidf_features : 2024-11-26 01:47:51.717406 - End\n",
            "train : (283366, 224) - End\n",
            "test : (231845, 223) - End\n",
            "add_embedding_features : 2024-11-26 01:49:38.101329 - End\n",
            "train : (283366, 324) - End\n",
            "test : (231845, 323) - End\n",
            "add_polarity_features : 2024-11-26 01:51:10.248086 - End\n",
            "train : (283366, 326) - End\n",
            "test : (231845, 325) - End\n",
            "Positive_Review : 2024-11-26 01:51:10.248225 - End\n",
            "add_bow_features  : 2024-11-26 01:51:16.192020 - End \n",
            "train : (283366, 336) - End\n",
            "test : (231845, 335) - End\n",
            "add_ngram_features : 2024-11-26 01:51:36.725913 - End\n",
            "train : (283366, 436) - End\n",
            "test : (231845, 435) - End\n"
          ]
        }
      ],
      "source": [
        "# Positive_ReviewとNegative_Reviewの内容をいろいろな手法で分析する.\n",
        "# BOWN,N-gram,TF-IDFの実行、埋め込み表現　・・・\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "def preprocess_reviews(train, test, columns):\n",
        "\n",
        "    # Bag-of-Words特徴量の追加　add_bow_features関数\n",
        "    \"\"\"\n",
        "    CountVectorizerを使用して、テキストデータを単語の出現回数に基づいた数値ベクトルに変換します。\n",
        "    stop_words='english'で英語のストップワード（一般的な単語）を削除します。\n",
        "    max_features=100で出現頻度が高い上位100単語のみを特徴量として使用します。\n",
        "    生成されたBag-of-Words特徴量は、trainとtestデータフレームに新しいカラムとして追加されます。\n",
        "    \"\"\"\n",
        "    def add_bow_features(train, test, column, max_features=10):\n",
        "        # Bag of Words特徴量の追加\n",
        "        bow = CountVectorizer(stop_words='english', max_features=10)    #max_features=50減らす\n",
        "        # 既にクリーニング済みのカラムデータをそのまま使用\n",
        "        train_bow = bow.fit_transform(train[column]).toarray()  # 変更箇所\n",
        "        test_bow = bow.transform(test[column]).toarray()       # 変更箇所\n",
        "\n",
        "        for i in range(train_bow.shape[1]):\n",
        "            train[f'{column}_bow_{i}'] = train_bow[:, i]\n",
        "            test[f'{column}_bow_{i}'] = test_bow[:, i]\n",
        "\n",
        "        return train, test\n",
        "\n",
        "\n",
        "    # テキストデータから N-gram 特徴量を抽出し、データフレームに追加する関数の定義\n",
        "    \"\"\"\n",
        "    N-gram とは、テキスト中に出現する連続した単語の組み合わせのことです。例えば、「the quick brown fox」という文から、\n",
        "        uni-gram (1-gram): the, quick, brown, fox\n",
        "        bi-gram (2-gram): the quick, quick brown, brown fox\n",
        "        tri-gram (3-gram): the quick brown, quick brown fox\n",
        "    といった N-gram を抽出できます。これらの N-gram を特徴量として使用することで、機械学習モデルはテキストデータのより深い意味を理解できるようになります。\n",
        "    CountVectorizer を使用し、uni-gram と bi-gram を抽出することで、単語の出現頻度だけでなく、単語の組み合わせも考慮した特徴量を作成している。\n",
        "    この関数は、指定されたカラム (column) のテキストデータから N-gram 特徴量を抽出し、データフレームに追加します。\n",
        "    ngram_range: N-gram の範囲を指定します。デフォルトは (1, 2) で、uni-gram と bi-gram を抽出します。\n",
        "    max_features: 追加する特徴量の上限を設定します。デフォルトは 100 で、出現頻度が高い上位 100 個の N-gram のみを特徴量として使用します。\n",
        "    \"\"\"\n",
        "    def add_ngram_features(df, column, ngram_range=(1, 2), max_features=50):  # max_features = 50\n",
        "\n",
        "        vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)  # max_features を設定\n",
        "        # 既にクリーニング済みのカラムデータをそのまま使用\n",
        "        ngram_features = vectorizer.fit_transform(df[column])  # 変更箇所\n",
        "\n",
        "        # 特徴量名をデータフレームに追加\n",
        "        ngram_feature_names = vectorizer.get_feature_names_out()  # 抽出された特徴量の名前を取得します。\n",
        "        for i, feature_name in enumerate(ngram_feature_names):\n",
        "            # 抽出された各 N-gram 特徴量を、新しいカラムとしてデータフレームに追加します。\n",
        "            df[f'{column}_ngram_{feature_name}'] = ngram_features[:, i].toarray().ravel()\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    # TF-IDF特徴量を追加する関数(定義)\n",
        "    \"\"\"\n",
        "    TF-IDFベクトライザーの適用: TfidfVectorizerを使用して、テキストデータをTF-IDF値に基づいた数値ベクトルに変換します。\n",
        "\n",
        "    max_features=1000で出現頻度が高い上位1000単語のみを特徴量として使用します。\n",
        "    stop_words='english'で英語のストップワードを削除します。\n",
        "    特徴量の次元削減: TruncatedSVDを使用して、TF-IDF特徴量の次元数を削減します。\n",
        "\n",
        "    n_components=20で次元数を20に削減します。\n",
        "    random_state=42で結果の再現性を確保します。\n",
        "    特徴量をデータフレームに追加: 生成されたTF-IDF特徴量は、trainとtestデータフレームに新しいカラムとして追加されます。\n",
        "\n",
        "    関数の適用: add_tfidf_features関数を'Positive_Review'と'Negative_Review'カラムに適用し、TF-IDF特徴量を追加します。\n",
        "    \"\"\"\n",
        "    def add_tfidf_features(train, test, column, n_components=100, max_features=100):\n",
        "\n",
        "        # TF-IDFベクトライザーの適用\n",
        "        tfidf = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
        "        tfidf_train = tfidf.fit_transform(train[column])\n",
        "        tfidf_test = tfidf.transform(test[column])\n",
        "\n",
        "        # 特徴量の次元削減\n",
        "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "        svd_train = svd.fit_transform(tfidf_train)\n",
        "        svd_test = svd.transform(tfidf_test)\n",
        "\n",
        "        # 特徴量をデータフレームに追加\n",
        "        for i in range(n_components):\n",
        "            train[f'TFIDF_{column}_{i}'] = svd_train[:, i]\n",
        "            test[f'TFIDF_{column}_{i}'] = svd_test[:, i]\n",
        "\n",
        "        return train, test\n",
        "\n",
        "\n",
        "    # 埋め込み表現をデータフレームに追加する関数定義\n",
        "    def add_embedding_features(df, column, model_name='glove-wiki-gigaword-100', max_features=50): # max_features を引数に追加\n",
        "\n",
        "        # glove-wiki-gigaword-100 という名前の事前学習済みモデル (GloVe) を読み込みます。\n",
        "        # このモデルは、Wikipediaなどの大規模なテキストデータから単語の埋め込み表現を学習しています。\n",
        "        model = api.load(model_name)  # 事前学習済みモデルの読み込み\n",
        "\n",
        "        # テキストを受け取り、その埋め込み表現を返す関数を定義\n",
        "        def get_embedding(text):\n",
        "            words = text.split()    # テキストを単語に分割\n",
        "            embeddings = [model[word] for word in words if word in model]   #各単語の埋め込み表現を事前学習済みモデルから取得\n",
        "\n",
        "            if embeddings:          # 単語の埋め込み表現が存在する場合、それらを平均してテキスト全体の埋め込み表現を計算\n",
        "                return np.mean(embeddings, axis=0)\n",
        "            else:                   # 単語の埋め込み表現が存在しない場合、ゼロベクトルを返す\n",
        "                return np.zeros(model.vector_size)\n",
        "\n",
        "        # get_embedding 関数を df の指定された column ( 'Positive_Review' または 'Negative_Review' ) に適用し、\n",
        "        # 埋め込み表現をデータフレームに変換する\n",
        "        embedding_features = df[column].apply(get_embedding).apply(pd.Series)\n",
        "\n",
        "        # 埋め込み表現の特徴量数を max_features で指定された数に制限します。\n",
        "        embedding_features = embedding_features.iloc[:, :max_features] # 列数を制限\n",
        "\n",
        "        # 埋め込み表現のカラムに名前を付けます (例: 'Positive_Review_embedding_0', 'Positive_Review_embedding_1', ...)。\n",
        "        embedding_features.columns = [f'{column}_embedding_{i}' for i in range(embedding_features.shape[1])]\n",
        "\n",
        "        # 埋め込み表現を元のデータフレームに結合します。\n",
        "        df = pd.concat([df, embedding_features], axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    # 極性と主観性をデータフレームに追加する関数の定義 合計4カラム追加される。\n",
        "    def add_polarity_features(df, column):\n",
        "\n",
        "        # TextBlob を使って、指定された column ( 'Positive_Review' または 'Negative_Review' ) のテキストの極性を計算し、{column}_polarity という名前の新しいカラムに追加します。\n",
        "        df[f'{column}_polarity'] = df[column].apply(lambda x: TextBlob(column).sentiment.polarity)\n",
        "\n",
        "        # 同様に、主観性を計算し、{column}_subjectivity という名前の新しいカラムに追加します。\n",
        "        df[f'{column}_subjectivity'] = df[column].apply(lambda x: TextBlob(column).sentiment.subjectivity)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    # 5種の関数を呼び出す ['Positive_Review', 'Negative_Review']の二回\n",
        "    \"\"\"\n",
        "    add_ngram_features 関数: データフレーム df に直接カラムを追加するため、return df がなくても元のデータフレームに反映される。\n",
        "    add_embedding_features 関数: 新しいデータフレームを作成して df に代入するため、return df で更新された df を返す必要がある。\n",
        "    \"\"\"\n",
        "    for col in columns:\n",
        "\n",
        "        # add_bow_features関数実行\n",
        "        train, test = add_bow_features(train, test, col, max_features=100)\n",
        "        print(f\"add_bow_features  : {datetime.now()} - End \")\n",
        "        print(f\"train : {train.shape} - End\")\n",
        "        print(f\"test : {test.shape} - End\")\n",
        "\n",
        "        # N-gram 特徴量を抽出関数実行\n",
        "        train = add_ngram_features(train, col, max_features=100)    # 使用例: 上位 〇〇 個の Ngram 特徴量のみを追加\n",
        "        test = add_ngram_features(test, col, max_features=100)     # 使用例: 上位 〇〇 個の Ngram 特徴量のみを追加\n",
        "        print(f\"add_ngram_features : {datetime.now()} - End\")\n",
        "        print(f\"train : {train.shape} - End\")\n",
        "        print(f\"test : {test.shape} - End\")\n",
        "\n",
        "        # TF-IDF特徴量追加関数実行 +200\n",
        "        train, test = add_tfidf_features(train, test, col, max_features=100)\n",
        "        print(f\"add_tfidf_features : {datetime.now()} - End\")\n",
        "        print(f\"train : {train.shape} - End\")\n",
        "        print(f\"test : {test.shape} - End\")\n",
        "\n",
        "        # 埋め込み表現追加関数実行 +200\n",
        "        train = add_embedding_features(train, col, max_features=100)\n",
        "        test = add_embedding_features(test, col, max_features=100)\n",
        "        print(f\"add_embedding_features : {datetime.now()} - End\")\n",
        "        print(f\"train : {train.shape} - End\")\n",
        "        print(f\"test : {test.shape} - End\")\n",
        "\n",
        "        # 極性と主観性を追加する関数実行 +4\n",
        "        train = add_polarity_features(train, col)\n",
        "        test = add_polarity_features(test, col)\n",
        "        print(f\"add_polarity_features : {datetime.now()} - End\")\n",
        "        print(f\"train : {train.shape} - End\")\n",
        "        print(f\"test : {test.shape} - End\")\n",
        "\n",
        "        print(f\"{col} : {datetime.now()} - End\")\n",
        "\n",
        "    # preprocess_reviews関数終わり\n",
        "    return train, test\n",
        "\n",
        "# preprocess_reviews関数の実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train, test = preprocess_reviews(train, test, review_columns)\n",
        "\n",
        "\n",
        "# キーワードを抽出し、特徴量として追加する関数　⁺100\n",
        "def preprocess_reviews2(df, max_features=100):\n",
        "    def extract_keywords(text):\n",
        "        words = text.lower().split()\n",
        "        common_words = ['the', 'and', 'is', 'in', 'it', 'to', 'was', 'i', 'of', 'a', 'this']\n",
        "        keywords = [word for word in words if word not in common_words]\n",
        "        return ' '.join(keywords)\n",
        "\n",
        "    df['Positive_Review_Keywords'] = df['Positive_Review'].apply(extract_keywords)\n",
        "    df['Negative_Review_Keywords'] = df['Negative_Review'].apply(extract_keywords)\n",
        "\n",
        "    # CountVectorizerを使ってキーワードをエンコード（フィーチャースペースを制限）\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "\n",
        "    positive_keywords_matrix = vectorizer.fit_transform(df['Positive_Review_Keywords'])\n",
        "    negative_keywords_matrix = vectorizer.fit_transform(df['Negative_Review_Keywords'])\n",
        "\n",
        "    # スパース行列を結合\n",
        "    keywords_matrix = hstack([positive_keywords_matrix, negative_keywords_matrix])\n",
        "\n",
        "    # スパース行列をデータフレームに変換する\n",
        "    keywords_df = pd.DataFrame(keywords_matrix.toarray(), columns=[f'Keyword_{i}' for i in range(keywords_matrix.shape[1])])\n",
        "\n",
        "    # 元のデータフレームに結合\n",
        "    df = pd.concat([df.reset_index(drop=True), keywords_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# キーワードを抽出し、特徴量として追加する関数の実行\n",
        "train = preprocess_reviews2(train, max_features=100)\n",
        "test = preprocess_reviews2(test, max_features=100)\n",
        "print(f\"preprocess_reviews2 : {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End  : {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessingTime: {processing_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49hSgOoNQgRE"
      },
      "outputs": [],
      "source": [
        "#Review項目以外の特徴量追加　#3、#4，#5\n",
        "#レビュー日付、ホテル名、緯度経度といった情報から新しい特徴量を生成\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "# 3. レビュー日付からの特徴量生成(定義)g\n",
        "def extract_date_features(df, date_column):\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "    df['review_weekday'] = df[date_column].dt.weekday   #レビューが投稿された曜日 (0:月曜日, 1:火曜日, ..., 6:日曜日)\n",
        "    df['review_month'] = df[date_column].dt.month       #レビューが投稿された月 (1:1月, 2:2月, ..., 12:12月)\n",
        "    df['review_quarter'] = df[date_column].dt.quarter   #レビューが投稿された四半期 (1:1-3月, 2:4-6月, 3:7-9月, 4:10-12月)\n",
        "    df['review_season'] = df[date_column].dt.month % 12 // 3 + 1    #レビューが投稿された季節 (1:春, 2:夏, 3:秋, 4:冬)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 3．レビュー日付からの特徴量生成(実行)\n",
        "date_column = 'Review_Date'\n",
        "train = extract_date_features(train, date_column)\n",
        "test = extract_date_features(test, date_column)\n",
        "print(f\"extract_date_features End:  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "# 4. ホテル名の処理(定義)\n",
        "def preprocess_hotel_names(train, test, column):\n",
        "    train[f'{column}_length'] = train[column].apply(len)    #Hotel_Name_length: ホテル名の文字数\n",
        "    test[f'{column}_length'] = test[column].apply(len)      #Hotel_Name_length: ホテル名の文字数\n",
        "\n",
        "    train[f'{column}_frequency'] = train[column].map(train[column].value_counts())  #Hotel_Name_frequency: データセット全体におけるそのホテル名の出現頻度\n",
        "    test[f'{column}_frequency'] = test[column].map(train[column].value_counts())    #Hotel_Name_frequency: データセット全体におけるそのホテル名の出現頻度\n",
        "\n",
        "    return train, test\n",
        "\n",
        "# 4．ホテル名の処理(実行)\n",
        "hotel_name_column = 'Hotel_Name'\n",
        "train, test = preprocess_hotel_names(train, test, hotel_name_column)\n",
        "print(f\"preprocess_hotel_names End:  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "# 5. 緯度・経度の組み合わせ(定義)\n",
        "#機械学習モデルがホテルの位置情報をより効果的に利用できるようにするために設計\n",
        "#train、testに、lat_lng と lat_lng_encoded という2つの新しいカラムを追加する\n",
        "\"\"\"\n",
        "1.緯度経度の結合:\n",
        "lat カラム（緯度）と lng カラム（経度）の値を文字列に変換し、\"_\" で連結して新しい lat_lng カラムを作成します。\n",
        "例えば、緯度が35.6895、経度が139.6917の場合、lat_lng カラムの値は \"35.6895_139.6917\" となります。\n",
        "これにより、緯度と経度を組み合わせた単一の特徴量が生成されます。\n",
        "2.ラベルエンコーディング:\n",
        "LabelEncoder を使用して、lat_lng カラムの値を数値に変換します。\n",
        "LabelEncoder は、カテゴリカルデータを数値に変換する際に使用される手法です。\n",
        "fit_transform メソッドを使用して、train データフレームの lat_lng カラムに含まれるユニークな値にそれぞれ数値を割り当てます。\n",
        "transform メソッドを使用して、test データフレームの lat_lng カラムの値を、train データフレームで割り当てられた数値に変換します。\n",
        "これにより、lat_lng_encoded という新しいカラムが作成され、緯度経度の組み合わせが数値で表現されます。\n",
        "\"\"\"\n",
        "def preprocess_lat_lng(train, test, lat_column, lng_column):\n",
        "    # K-meansクラスタリングの代わりに、簡単な組み合わせ特徴量を作成\n",
        "    train['lat_lng'] = train[lat_column].astype(str) + '_' + train[lng_column].astype(str)\n",
        "    test['lat_lng'] = test[lat_column].astype(str) + '_' + test[lng_column].astype(str)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    train['lat_lng_encoded'] = encoder.fit_transform(train['lat_lng'])\n",
        "    test['lat_lng_encoded'] = encoder.transform(test['lat_lng'])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "# 5．緯度・経度の組み合わせ(実行)\n",
        "lat_column = 'lat'\n",
        "lng_column = 'lng'\n",
        "train, test = preprocess_lat_lng(train, test, lat_column, lng_column)\n",
        "print(f\"preprocess_lat_lng End:  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "\n",
        "\n",
        "#一番古いコーディング部分\n",
        "def preprocess_data(df):\n",
        "\n",
        "    # レビューの文字数を計算する\n",
        "    df['Positive_Review_Length'] = df['Positive_Review'].apply(len)\n",
        "    df['Negative_Review_Length'] = df['Negative_Review'].apply(len)\n",
        "\n",
        "    # レビューのポジティブ・ネガティブ比率\n",
        "    df['Review_Word_Ratio'] = df['Positive_Review_Length'] / (df['Negative_Review_Length'] + 1)\n",
        "\n",
        "    # ポジティブレビューの単語数を計算\n",
        "    df['Positive_Review_Word_Count'] = df['Positive_Review'].apply(lambda x: len(x.split()))\n",
        "    # ネガティブレビューの単語数を計算\n",
        "    df['Negative_Review_Word_Count'] = df['Negative_Review'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    # VADERを使ったセンチメント分析\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    df['Positive_Review_Sentiment'] = df['Positive_Review'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "    df['Negative_Review_Sentiment'] = df['Negative_Review'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "\n",
        "    # 過去のレビュー数と平均スコア\n",
        "    df['Reviews_Per_Hotel'] = df.groupby('Hotel_Name')['Total_Number_of_Reviews'].transform('sum')\n",
        "    df['Average_Score_Per_Hotel'] = df.groupby('Hotel_Name')['Average_Score'].transform('mean')\n",
        "\n",
        "    # ホテルのレビュー数の変化\n",
        "    df['Review_Trend'] = df.groupby('Hotel_Name')['Total_Number_of_Reviews'].transform(lambda x: x.diff().fillna(0))\n",
        "    df['Average_Score_Trend'] = df.groupby('Hotel_Name')['Average_Score'].transform(lambda x: x.diff().fillna(0))\n",
        "\n",
        "    # レビュアーのアクティブ度\n",
        "    df['Reviewer_Activity'] = df['Total_Number_of_Reviews_Reviewer_Has_Given'] / df['days_since_review'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "    # 月ごとの宿泊者満足度\n",
        "    if 'Reviewer_Score' in df.columns:\n",
        "        df['Monthly_Avg_Score'] = df.groupby('review_month')['Reviewer_Score'].transform('mean')\n",
        "\n",
        "    # 否定的・肯定的なレビューの有無\n",
        "    df['Has_Negative_Review'] = df['Negative_Review'].apply(lambda x: 1 if x.strip() else 0)\n",
        "    df['Has_Positive_Review'] = df['Positive_Review'].apply(lambda x: 1 if x.strip() else 0)\n",
        "\n",
        "    # レビュアーの詳細情報解析\n",
        "    df['Trip_Type'] = df['Tags'].apply(lambda x: 1 if 'Leisure trip' in x else 0)\n",
        "    df['With_Family'] = df['Tags'].apply(lambda x: 1 if 'Family with young children' in x or 'Family with older children' in x else 0)\n",
        "    df['With_Partner'] = df['Tags'].apply(lambda x: 1 if 'Couple' in x else 0)\n",
        "    df['Solo_Traveler'] = df['Tags'].apply(lambda x: 1 if 'Solo traveler' in x else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "#一番古いコーディング部分の実行\n",
        "train = preprocess_data(train)\n",
        "test = preprocess_data(test)\n",
        "print(f\"preprocess_data End:  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "#11月25日特徴量追加\n",
        "\n",
        "#時間特徴量の生成\n",
        "# レビューの時間帯（朝、昼、夜）を特徴量として追加\n",
        "def add_time_features(df, date_column):\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "    df['review_hour'] = df[date_column].dt.hour\n",
        "    df['review_time_of_day'] = df['review_hour'].apply(lambda x: 'morning' if 5 <= x < 12 else ('afternoon' if 12 <= x < 17 else 'evening'))\n",
        "    return df\n",
        "# 実行\n",
        "date_column = 'Review_Date'\n",
        "train = add_time_features(train, date_column)\n",
        "test = add_time_features(test, date_column)\n",
        "print(f\"add_time_features :  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "# テキストの読みやすさ指標\n",
        "!pip install textstat\n",
        "from textstat import textstat\n",
        "# テキストの読みやすさ指標を追加\n",
        "def add_readability_features(df, columns):\n",
        "    for col in columns:\n",
        "        df[f'{col}_flesch_reading_ease'] = df[col].apply(lambda x: textstat.flesch_reading_ease(x) if pd.notnull(x) else 0)\n",
        "    return df\n",
        "# 実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train = add_readability_features(train, review_columns)\n",
        "test = add_readability_features(test, review_columns)\n",
        "print(f\"add_readalility_features:  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "#クラスタリングによる特徴量\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "# テキストデータを数値ベクトルに変換してからクラスタリングを行います\n",
        "def add_clustering_features(df, columns, n_clusters=5):\n",
        "    for col in columns:\n",
        "        # TF-IDFベクトル化\n",
        "        tfidf_vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(df[col].fillna(''))\n",
        "\n",
        "        # KMeansクラスタリング\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        df[f'{col}_cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "    return df\n",
        "# 実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train = add_clustering_features(train, review_columns)\n",
        "test = add_clustering_features(test, review_columns)\n",
        "print(f\"add_clustering_features :  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "\"\"\"\n",
        "#エンティティ認識　→時間かかる\n",
        "import spacy\n",
        "\n",
        "# SpaCyを使用してエンティティを抽出し、特徴量として追加\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def add_entity_features(df, columns):\n",
        "    for col in columns:\n",
        "        df[f'{col}_entities'] = df[col].apply(lambda x: ' '.join([ent.label_ for ent in nlp(x).ents]) if pd.notnull(x) else '')\n",
        "    return df\n",
        "# 実行\n",
        "review_columns = ['Positive_Review', 'Negative_Review']\n",
        "train = add_entity_features(train, review_columns)\n",
        "test = add_entity_features(test, review_columns)\n",
        "print(f\"add_entity features :  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\"\"\"\n",
        "\n",
        "# 既存の特徴量の積や商などの相互作用項を生成\n",
        "def add_interaction_features(df, columns):\n",
        "    for i in range(len(columns)):\n",
        "        for j in range(i + 1, len(columns)):\n",
        "            col1 = columns[i]\n",
        "            col2 = columns[j]\n",
        "            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
        "            df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-5)  # ゼロ除算を防ぐために小さな値を追加\n",
        "    return df\n",
        "# 実行\n",
        "numeric_columns = ['review_hour', 'review_weekday', 'review_month', 'review_quarter', 'review_season']\n",
        "train = add_interaction_features(train, numeric_columns)\n",
        "test = add_interaction_features(test, numeric_columns)\n",
        "print(f\"add_interaction_features : {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "\n",
        "\n",
        "#4. タグの埋め込み表現\n",
        "from gensim.models import Word2Vec\n",
        "def add_tag_embeddings(df, tag_column, vector_size=50):\n",
        "    tags = df[tag_column].dropna().apply(lambda x: x.split(',')).tolist()\n",
        "    model = Word2Vec(tags, vector_size=vector_size, window=5, min_count=1, sg=1)\n",
        "    def get_tag_vector(tags):\n",
        "        tag_vecs = [model.wv[tag] for tag in tags if tag in model.wv]\n",
        "        if tag_vecs:\n",
        "            return np.mean(tag_vecs, axis=0)\n",
        "        else:\n",
        "            return np.zeros(vector_size)\n",
        "    tag_vectors = df[tag_column].apply(lambda x: get_tag_vector(x.split(',')) if pd.notnull(x) else np.zeros(vector_size))\n",
        "    tag_vectors_df = pd.DataFrame(tag_vectors.tolist(), index=df.index)\n",
        "    return pd.concat([df, tag_vectors_df], axis=1)\n",
        "#実行\n",
        "train = add_tag_embeddings(train, 'Tags')\n",
        "test = add_tag_embeddings(test, 'Tags')\n",
        "print(f\"add_tag_embeddings :  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "#タグのクラスタリング\n",
        "from sklearn.cluster import KMeans\n",
        "def add_tag_clusters(df, tag_column, n_clusters=5):\n",
        "    vectorizer = CountVectorizer(tokenizer=lambda x: x.split(','))\n",
        "    tag_matrix = vectorizer.fit_transform(df[tag_column].fillna(''))\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(tag_matrix)\n",
        "    df[tag_column + '_cluster'] = kmeans.labels_\n",
        "    return df\n",
        "#実行\n",
        "train = add_tag_clusters(train, 'Tags')\n",
        "test = add_tag_clusters(test, 'Tags')\n",
        "print(f\"add_tag_clusters :  {datetime.now()} - End\")\n",
        "print(f\"train : {train.shape} - End\")\n",
        "print(f\"test : {test.shape} - End\")\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End  : {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYLWwxfjReWs"
      },
      "outputs": [],
      "source": [
        "#6まで全体のバックアップ取得\n",
        "train_bkup6 = train.copy()\n",
        "test_bkup6 = test.copy()\n",
        "\n",
        "print(train_bkup6.shape)\n",
        "print(test_bkup6.shape)\n",
        "\n",
        "#(283366, 327)\n",
        "#(231845, 325)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ddnBwQZ-jn"
      },
      "outputs": [],
      "source": [
        "#バックアップから戻す\n",
        "train = train_bkup6.copy()\n",
        "test = test_bkup6.copy()\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMr6ZGJi5YgH"
      },
      "outputs": [],
      "source": [
        "print(train.info())\n",
        "print(test.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYyiHczWfOvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_pbMHmqfO65"
      },
      "outputs": [],
      "source": [
        "#LightGBM単品コード書き直し　11月26日\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "# 訓練用データセットの数値型以外の列を削除する\n",
        "train = train.select_dtypes(include=['int64', 'float64', 'int32'])\n",
        "\n",
        "# 推論用データセットにもレビュースコア以外の同様の列を残す\n",
        "test = test[[col for col in train.columns if col in test.columns]]\n",
        "\n",
        "# 訓練用データセットからターゲットを分離する\n",
        "X = train.drop('Reviewer_Score', axis=1)\n",
        "y = train['Reviewer_Score']\n",
        "\n",
        "# テストデータに存在しない列を追加して0で埋める\n",
        "missing_cols = set(X.columns) - set(test.columns)\n",
        "for col in missing_cols:\n",
        "    test[col] = 0\n",
        "\n",
        "# 列の順番を訓練データと同じにする\n",
        "test = test[X.columns]\n",
        "\n",
        "# 無限大の値を置換\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 欠損値を平均値で補完 (他の方法でも補完可能)\n",
        "X = X.fillna(X.mean())            #平均値で埋める\n",
        "test = test.fillna(test.mean())\n",
        "# 例：X_train = X_train.dropna()  # NaN値を含む行を削除\n",
        "\n",
        "# float64に型変換\n",
        "X = X.astype(np.float64)\n",
        "test = test.astype(np.float64)\n",
        "\n",
        "# KFoldによる分割設定\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 結果を保存するリスト\n",
        "rmses = []\n",
        "test_preds_lgb = np.zeros(test.shape[0])\n",
        "\n",
        "# 各モデルの最適なハイパーパラメータの設定\n",
        "lgb_params = {\n",
        "    'learning_rate': 0.0992812083091482,   #0.05から変更\n",
        "    'min_child_samples': 61,    #\n",
        "    'n_estimators': 1000,   #500から変更\n",
        "    'num_leaves': 250,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# 各foldでの訓練と評価\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X)):\n",
        "    print(f\"Fold {fold+1} Start:{datetime.now()}\")\n",
        "\n",
        "    # 訓練用データと検証用データに分割\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "\n",
        "    # LightGBMモデルの訓練\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "    print(f\"Fold {fold+1} LightGBM time:{datetime.now()}\")\n",
        "\n",
        "\n",
        "    # 検証用データセットに対する予測\n",
        "    y_pred_lgb = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
        "\n",
        "    # モデルの性能をRMSEで評価\n",
        "    rmse_lgb = mean_squared_error(y_valid, y_pred_lgb, squared=False)\n",
        "    print(f\"Fold {fold+1} RMSE (LightGBM): {rmse_lgb:.4f}\")\n",
        "\n",
        "    # 各foldの結果を保存\n",
        "    rmses.append((rmse_lgb,))    # 単一要素のタプルとして追加\n",
        "\n",
        "    # テストデータに対する予測を平均\n",
        "    test_preds_lgb += lgb_model.predict(test, num_iteration=lgb_model.best_iteration) / kf.n_splits\n",
        "\n",
        "# 全体の平均RMSE\n",
        "mean_rmse_lgb = sum([rmse[0] for rmse in rmses]) / len(rmses)\n",
        "\n",
        "print(f\"Mean RMSE (LightGBM): {mean_rmse_lgb:.4f}\")\n",
        "\n",
        "# アンサンブル予測の平均\n",
        "final_test_preds = (test_preds_lgb) / 1\n",
        "\n",
        "# 予測結果を保存する\n",
        "submit = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv', header=None)\n",
        "submit[1] = final_test_preds\n",
        "submit.to_csv('/content/drive/MyDrive/data/submit_LightGBM1.csv', header=None, index=False)\n",
        "\n",
        "submit.head()\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End: {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ... (LightGBM モデルの学習) ...\n",
        "\n",
        "# SHAP の Explainer オブジェクトを作成\n",
        "explainer = shap.TreeExplainer(lgb_model)\n",
        "\n",
        "# SHAP 値を計算\n",
        "shap_values = explainer.shap_values(test_selected) # X_test は説明したいデータ\n",
        "\n",
        "# SHAP の summary plot を表示\n",
        "shap.summary_plot(shap_values, test_selected)"
      ],
      "metadata": {
        "id": "K_NkCd0rxHIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特徴量重要度を取得\n",
        "feature_importance = lgb_model.feature_importance()  # 既存のモデルから重要度を取得\n",
        "feature_importance_df = pd.DataFrame({'feature': lgb_model.feature_name(), 'importance': feature_importance})\n",
        "\n",
        "# 重要度でソート\n",
        "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "# ランキングを表示\n",
        "print(feature_importance_df)  # DataFrame全体を表示\n",
        "\n",
        "# 上位N件のみ表示したい場合 (例: 上位50件)\n",
        "N = 200\n",
        "print(feature_importance_df.head(N))\n",
        "\n",
        "# 重要度の低い特徴量を削除 (例: 重要度が下位20%の特徴量を削除)\n",
        "threshold = feature_importance_df['importance'].quantile(0.2)\n",
        "selected_features = feature_importance_df[feature_importance_df['importance'] > threshold]['feature'].tolist()\n",
        "\n",
        "# selected_features に含まれるカラムが X に存在するか確認\n",
        "# 存在しないカラムがあれば警告を表示し、selected_features から削除\n",
        "missing_features = [f for f in selected_features if f not in X.columns]\n",
        "if missing_features:\n",
        "    print(f\"警告: 以下の特徴量は X に存在しません: {missing_features}\")\n",
        "    selected_features = [f for f in selected_features if f in X.columns]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 選択された特徴量のみでデータを準備\n",
        "X_selected = X[selected_features]  # Xは特徴量データ\n",
        "test_selected = test[selected_features]\n",
        "\n",
        "# test_selected に不足している列を取得する\n",
        "missing_cols = set(X_selected.columns) - set(test_selected.columns)\n",
        "\n",
        "# test_selected に不足している列を追加し、0 で埋める\n",
        "for col in missing_cols:\n",
        "    test[col] = 0\n",
        "\n",
        "# 列の順序が同じであることを確認する\n",
        "test_selected = test_selected[X_selected.columns]\n",
        "\n",
        "\n",
        "# KFoldによる分割設定\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 結果を保存するリスト\n",
        "rmses = []\n",
        "test_preds = np.zeros(test_selected.shape[0])\n",
        "\n",
        "# 各foldでの訓練と評価(2回目)\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X)):\n",
        "    print(f\"2回目のFold {fold+1} Start:{datetime.now()}\")\n",
        "\n",
        "    # 訓練用データと検証用データに分割 (選択された特徴量を使用)\n",
        "    X_train, X_valid = X_selected.iloc[train_index], X_selected.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]  # yはターゲット変数\n",
        "\n",
        "    # LightGBM用のデータセットに変換 (選択された特徴量を使用)\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
        "\n",
        "\n",
        "    # LightGBMモデルの訓練\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "    print(f\"2回目のFold {fold+1} LightGBM time:{datetime.now()}\")\n",
        "\n",
        "\n",
        "    # 検証用データセットに対する予測\n",
        "    y_pred_lgb = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
        "\n",
        "    # モデルの性能をRMSEで評価\n",
        "    rmse_lgb = mean_squared_error(y_valid, y_pred_lgb, squared=False)\n",
        "    print(f\"2回目のFold {fold+1} RMSE (LightGBM): {rmse_lgb:.4f}\")\n",
        "\n",
        "    # 各foldの結果を保存\n",
        "    rmses.append((rmse_lgb,))    # 単一要素のタプルとして追加\n",
        "\n",
        "    # テストデータに対する予測を平均\n",
        "    test_preds_lgb += lgb_model.predict(test_selected, num_iteration=lgb_model.best_iteration) / kf.n_splits\n",
        "\n",
        "# 全体の平均RMSE\n",
        "mean_rmse_lgb = sum([rmse[0] for rmse in rmses]) / len(rmses)\n",
        "\n",
        "print(f\"2回目のMean RMSE (LightGBM): {mean_rmse_lgb:.4f}\")\n",
        "\n",
        "# アンサンブル予測の平均\n",
        "final_test_preds = (test_preds_lgb) / 1\n",
        "\n",
        "# 予測結果を保存する\n",
        "submit = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv', header=None)\n",
        "submit[1] = final_test_preds\n",
        "submit.to_csv('/content/drive/MyDrive/data/submit_LightGBM2.csv', header=None, index=False)\n",
        "feature_importance_df.to_csv('/content/drive/MyDrive/data/feature_importance.csv', index=False)\n",
        "submit.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I9fcTpOXlMWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_df.to_csv('/content/drive/MyDrive/data/feature_importance.csv', index=False)\n"
      ],
      "metadata": {
        "id": "pH2rFXFauVde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XScmbuIHdpD"
      },
      "outputs": [],
      "source": [
        "#アンサンブルコード書き直し　11月25日\n",
        "#ランダムフォレスト除外\n",
        "#3つのモデルのハイパーパラメータ変更\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "# 必要なモジュールのインポートを追加→結局使わない\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "# 訓練用データセットの数値型以外の列を削除する\n",
        "train = train.select_dtypes(include=['int64', 'float64', 'int32'])\n",
        "\n",
        "# 推論用データセットにもレビュースコア以外の同様の列を残す\n",
        "test = test[[col for col in train.columns if col in test.columns]]\n",
        "\n",
        "# 訓練用データセットからターゲットを分離する\n",
        "X = train.drop('Reviewer_Score', axis=1)\n",
        "y = train['Reviewer_Score']\n",
        "\n",
        "# テストデータに存在しない列を追加して0で埋める\n",
        "missing_cols = set(X.columns) - set(test.columns)\n",
        "for col in missing_cols:\n",
        "    test[col] = 0\n",
        "\n",
        "\n",
        "# 列の順番を訓練データと同じにする\n",
        "test = test[X.columns]\n",
        "\n",
        "# 無限大の値を置換\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 欠損値を平均値で補完 (他の方法でも補完可能)\n",
        "X = X.fillna(X.mean())            #平均値で埋める\n",
        "test = test.fillna(test.mean())\n",
        "# 例：X_train = X_train.dropna()  # NaN値を含む行を削除\n",
        "\n",
        "# float64に型変換\n",
        "X = X.astype(np.float64)\n",
        "test = test.astype(np.float64)\n",
        "\n",
        "\n",
        "# KFoldによる分割設定\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 結果を保存するリスト\n",
        "rmses = []\n",
        "test_preds_lgb = np.zeros(test.shape[0])\n",
        "test_preds_rf = np.zeros(test.shape[0])\n",
        "test_preds_xgb = np.zeros(test.shape[0])\n",
        "test_preds_cat = np.zeros(test.shape[0])\n",
        "\n",
        "# 各モデルの最適なハイパーパラメータの設定\n",
        "lgb_params = {\n",
        "    'learning_rate': 0.0992812083091482,   #0.05から変更\n",
        "    'min_child_samples': 61,    #\n",
        "    'n_estimators': 1000,   #500から変更\n",
        "    'num_leaves': 250,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "#Optunaで算出したパラメータ\n",
        "#Best parameters: {'learning_rate': 0.0992812083091482, 'num_leaves': 250, 'min_child_samples': 61}\n",
        "#Best RMSE: 1.012224322083124\n",
        "\"\"\"\n",
        "rf_params = {\n",
        "    'n_estimators': 25, #ランダムフォレストを構成する決定木の数,100〜200 程度から始め、交差検証などで最適な値を探索するのが良い\n",
        "    'random_state': 42, #乱数シード\n",
        "    'max_depth': 10,    #各決定木の最大深さ None（制限なし）から始め、交差検証などで最適な値を探索するのが良い\n",
        "    'min_samples_split': 2, #ノードを分割するために必要な最小サンプル数 2〜10 程度から始め、交差検証などで最適な値を探索するのが良い\n",
        "    'min_samples_leaf': 1,  #葉ノードに存在する最小サンプル数 1〜5 程度から始め、交差検証などで最適な値を探索するのが良い\n",
        "    #'max_features': 'auto',    #各決定木でノードを分割する際に考慮する特徴量の最大数 'auto'（特徴量の数の平方根）、'sqrt'、'log2' など\n",
        "    'n_jobs': -1    #並列処理に使用するコア数  -1 を指定すると、すべてのコアを使用\n",
        "}\n",
        "\"\"\"\n",
        "xgb_params = {\n",
        "    'learning_rate': 0.1,  #デフォルトは0.3　一般的に、0.01〜0.1程度の値が使用される。\n",
        "    'n_estimators': 1000,    #ブースター（決定木）の数\n",
        "    'objective': 'reg:squarederror',    #reg:squarederror（回帰）\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "cat_params = {\n",
        "    'iterations': 500,      #学習の反復回数 (決定木の数)\n",
        "    'learning_rate': 0.1,  #学習率\n",
        "    'depth': 6,             #決定木の最大深度\n",
        "    'loss_function': 'RMSE',#損失関数\n",
        "    'random_seed': 42,      #乱数シード\n",
        "    'verbose': 0            #学習過程の出力レベル　0表示なし　1表示あり\n",
        "}\n",
        "\n",
        "# 各foldでの訓練と評価\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X)):\n",
        "    print(f\"Fold {fold+1} Start:{datetime.now()}\")\n",
        "\n",
        "    # 訓練用データと検証用データに分割\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "\n",
        "    # LightGBMモデルの訓練\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "    print(f\"Fold {fold+1} LightGBM time:{datetime.now()}\")\n",
        "\n",
        "\n",
        "    # XGBoostモデルの訓練\n",
        "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "    early_stop = EarlyStopping(rounds=50) # EarlyStoppingコールバックを作成\n",
        "    #xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stop], verbose=False) # callbacks引数に追加\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
        "    print(f\"Fold {fold+1} XGBoost time:{datetime.now()}\")\n",
        "\n",
        "    # CatBoostモデルの訓練\n",
        "    cat_model = CatBoostRegressor(**cat_params)\n",
        "    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, verbose=False)\n",
        "    print(f\"Fold {fold+1} CatBoost time:{datetime.now()}\")\n",
        "\n",
        "    # ランダムフォレストモデルの訓練\n",
        "    #rf_model = RandomForestRegressor(**rf_params)\n",
        "    #rf_model.fit(X_train, y_train)\n",
        "    #print(f\"Fold {fold+1} ランダムフォレスト time:{datetime.now()}\")\n",
        "\n",
        "\n",
        "    # 検証用データセットに対する予測\n",
        "    y_pred_lgb = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
        "    y_pred_xgb = xgb_model.predict(X_valid)\n",
        "    y_pred_cat = cat_model.predict(X_valid)\n",
        "    #y_pred_rf = rf_model.predict(X_valid)\n",
        "\n",
        "    # モデルの性能をRMSEで評価\n",
        "    rmse_lgb = mean_squared_error(y_valid, y_pred_lgb, squared=False)\n",
        "    rmse_xgb = mean_squared_error(y_valid, y_pred_xgb, squared=False)\n",
        "    rmse_cat = mean_squared_error(y_valid, y_pred_cat, squared=False)\n",
        "    #rmse_rf = mean_squared_error(y_valid, y_pred_rf, squared=False)\n",
        "    print(f\"Fold {fold+1} RMSE (LightGBM): {rmse_lgb:.4f}\")\n",
        "    print(f\"Fold {fold+1} RMSE (XGBoost): {rmse_xgb:.4f}\")\n",
        "    print(f\"Fold {fold+1} RMSE (CatBoost): {rmse_cat:.4f}\")\n",
        "    #print(f\"Fold {fold+1} RMSE (RandomForest): {rmse_rf:.4f}\")\n",
        "\n",
        "    # 各foldの結果を保存\n",
        "    #rmses.append((rmse_lgb, rmse_rf, rmse_xgb, rmse_cat))\n",
        "    rmses.append((rmse_lgb, rmse_xgb, rmse_cat))\n",
        "\n",
        "    # テストデータに対する予測を平均\n",
        "    test_preds_lgb += lgb_model.predict(test, num_iteration=lgb_model.best_iteration) / kf.n_splits\n",
        "    test_preds_xgb += xgb_model.predict(test) / kf.n_splits\n",
        "    test_preds_cat += cat_model.predict(test) / kf.n_splits\n",
        "    #test_preds_rf += rf_model.predict(test) / kf.n_splits\n",
        "\n",
        "# 全体の平均RMSE\n",
        "mean_rmse_lgb = sum([rmse[0] for rmse in rmses]) / len(rmses)\n",
        "mean_rmse_xgb = sum([rmse[1] for rmse in rmses]) / len(rmses)\n",
        "mean_rmse_cat = sum([rmse[2] for rmse in rmses]) / len(rmses)\n",
        "#mean_rmse_rf = sum([rmse[3] for rmse in rmses]) / len(rmses)\n",
        "\n",
        "print(f\"Mean RMSE (LightGBM): {mean_rmse_lgb:.4f}\")\n",
        "print(f\"Mean RMSE (XGBoost): {mean_rmse_xgb:.4f}\")\n",
        "print(f\"Mean RMSE (CatBoost): {mean_rmse_cat:.4f}\")\n",
        "#print(f\"Mean RMSE (RandomForest): {mean_rmse_rf:.4f}\")\n",
        "\n",
        "# アンサンブル予測の平均\n",
        "#final_test_preds = (test_preds_lgb + test_preds_rf + test_preds_xgb + test_preds_cat) / 4\n",
        "final_test_preds = (test_preds_lgb + test_preds_xgb + test_preds_cat) / 3\n",
        "\n",
        "# 予測結果を保存する\n",
        "submit = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv', header=None)\n",
        "submit[1] = final_test_preds\n",
        "submit.to_csv('/content/drive/MyDrive/data/submit_ensemble.csv', header=None, index=False)\n",
        "\n",
        "submit.head()\n",
        "\n",
        "# 処理時間を計算\n",
        "end_time = datetime.now()\n",
        "print(f\"End: {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6m2wTB9lhHC"
      },
      "outputs": [],
      "source": [
        "\"\"\"コピー用\n",
        "start_time = datetime.now()\n",
        "print(f\"Start: {start_time}\")\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f\"End: {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfOhXUIh4vgv"
      },
      "source": [
        "以降は、チューニング用のモジュール\n",
        "Optuna、"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXY9uWQi46Yt"
      },
      "source": [
        "GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxM7toEDI56K"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import optuna\n",
        "\n",
        "#learning_rate, num_leaves, max_depthはモデルの性能に大きく影響するため、優先的にチューニングすることをお勧めします。\n",
        "def objective(trial):\n",
        "    # ハイパーパラメータの範囲を指定\n",
        "    param = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        # ... other hyperparameters ...\n",
        "    }\n",
        "\n",
        "    # LightGBMモデルの学習\n",
        "    lgb_model = lgb.train(\n",
        "        param,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "\n",
        "    # 予測とRMSEの計算\n",
        "    y_pred = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
        "    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "\n",
        "    return rmse  # 最小化したい指標を返す\n",
        "\n",
        "study = optuna.create_study(direction='minimize')  # 最小化したい場合は 'minimize'\n",
        "study.optimize(objective, n_trials=100)  # 試行回数\n",
        "\n",
        "# 最適なハイパーパラメータ\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jymgDjuqNfqc"
      },
      "outputs": [],
      "source": [
        "start_time = datetime.now()\n",
        "print(f\"Start xgb_model: {start_time}\")\n",
        "\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        # ... other hyperparameters ...\n",
        "    }\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(**param)\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
        "\n",
        "    y_pred = xgb_model.predict(X_valid)\n",
        "    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# 最適なハイパーパラメータ\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n",
        "\n",
        "\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f\"End xgb_model: {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9r4QonPNhAV"
      },
      "outputs": [],
      "source": [
        "start_time = datetime.now()\n",
        "print(f\"Start Cat: {start_time}\")\n",
        "\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'depth': trial.suggest_int('depth', 4, 10),\n",
        "        # ... other hyperparameters ...\n",
        "    }\n",
        "\n",
        "    cat_model = CatBoostRegressor(**param, verbose=False)\n",
        "    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
        "\n",
        "    y_pred = cat_model.predict(X_valid)\n",
        "    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# 最適なハイパーパラメータ\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n",
        "\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f\"End cat: {end_time}\")\n",
        "processing_time = end_time - start_time\n",
        "print(f\"ProcessTime: {processing_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d-GrfcwWkZU"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)\n",
        "    }\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, preds)\n",
        "    return mse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best parameters: \", study.best_params)\n",
        "print(\"Best MSE: \", study.best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b9mCfucL1KnG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# ハイパーパラメータの候補を設定\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 50, 70],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'min_child_samples': [20, 30, 50]\n",
        "}\n",
        "\n",
        "# LightGBMのモデルを設定\n",
        "lgb_model = lgb.LGBMRegressor(objective='regression', random_state=42)\n",
        "\n",
        "# GridSearchCVを使用して最適なハイパーパラメータを探索\n",
        "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# 最適なハイパーパラメータを表示\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_xUtRzkQIz"
      },
      "source": [
        "LightGBMのパラメータチューニングには以下の主要なパラメータがあります。これらはモデルの性能に大きな影響を与えるため、優先して調整すると良いでしょう。\n",
        "\n",
        "num_leaves: 決定木の最大葉数。大きくするとモデルの複雑さが増すが、過学習のリスクもある。\n",
        "\n",
        "learning_rate: 学習率。小さくすると学習が安定するが、計算時間が増える。\n",
        "\n",
        "n_estimators: 決定木の数。多くすると精度が上がるが、計算コストも増える。\n",
        "\n",
        "max_depth: 決定木の最大深度。深くするとモデルの複雑さが増すが、過学習のリスクもある。\n",
        "\n",
        "min_child_weight: 子ノードの最小サンプル数。小さくすると過学習しやすくなる。\n",
        "\n",
        "subsample: バギングの割合。過学習を防ぐためにデータの一部を使用する。\n",
        "\n",
        "colsample_bytree: 決定木ごとに使用する特徴量の割合。過学習を防ぐために特徴量の一部を使用する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjQ1mUbalLM5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msmq_JhJkhuZ"
      },
      "source": [
        "CatBoostのパラメータチューニングには、以下の主要なパラメータがあります。これらはモデルの性能に大きな影響を与えるため、優先して調整すると良いでしょう。\n",
        "\n",
        "iterations: 学習回数。多くすると精度が上がるが、計算コストも増える。\n",
        "\n",
        "learning_rate: 学習率。小さくすると学習が安定するが、計算時間が増える。\n",
        "\n",
        "depth: 決定木の深さ。深くするとモデルの複雑さが増すが、過学習のリスクもある。\n",
        "\n",
        "l2_leaf_reg: L2正則化項。大きくすると過学習を防ぐが、精度が下がる可能性がある。\n",
        "\n",
        "bagging_temperature: バギングの温度。大きくするとデータの一部を使用する確率が高くなる。\n",
        "\n",
        "border_count: 数値特徴量をカテゴリカルに変換する際の分割数。多くすると精度が上がるが、計算コストも増える。\n",
        "\n",
        "random_strength: 決定木の分割時にランダム性を追加する強さ。過学習を防ぐために使用する。\n",
        "\n",
        "これらのパラメータを調整することで、モデルの性能を最適化することができます。Optunaなどの自動化されたチューニングツールを使用して、効率的に最適なパラメータを見つけることをお勧めします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A14zrqWFlMhi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqr6bjzQlCES"
      },
      "source": [
        "XGBoostのパラメータチューニングには、以下の主要なパラメータがあります。これらはモデルの性能に大きな影響を与えるため、優先して調整すると良いでしょう。\n",
        "\n",
        "eta (learning_rate): 学習率。小さくすると学習が安定するが、計算時間が増える。\n",
        "\n",
        "max_depth: 決定木の深さ。深くするとモデルの複雑さが増すが、過学習のリスクもある。\n",
        "\n",
        "min_child_weight: 葉の重みの最小値。値を大きくすると過学習を防ぐが、精度が下がる可能性がある。\n",
        "\n",
        "subsample: 学習データのサンプリング率。値を小さくすると過学習を防ぐが、あまり小さくしすぎるとモデルの性能が低下する。\n",
        "\n",
        "colsample_bytree: 決定木ごとの特徴量のサンプリング率。値を小さくすると過学習を防ぐが、あまり小さくしすぎるとモデルの性能が低下する。\n",
        "\n",
        "gamma: ノード分割の基準となる損失減少の最小値。値を大きくすると過学習を防ぐが、精度が下がる可能性がある。\n",
        "\n",
        "lambda (reg_lambda): L2正則化項。大きくすると過学習を防ぐが、精度が下がる可能性がある。\n",
        "\n",
        "alpha (reg_alpha): L1正則化項。大きくすると過学習を防ぐが、精度が下がる可能性がある.\n",
        "\n",
        "これらのパラメータを調整することで、モデルの性能を最適化することができます。Optunaなどの自動化されたチューニングツールを使用して、効率的に最適なパラメータを見つけることをお勧めします。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMmEA+BpByn4KyvLzGqlOlm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}